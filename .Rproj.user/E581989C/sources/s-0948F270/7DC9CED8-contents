# Protests as Strategic Games - An interactive <br/>Analysis in R

<br/>

Author: Robert Laetsch

#< ignore
```{r "setup"}
run=function(){
  #load the libraries needed for R Tutor to work
  library(RTutor)
  library(yaml)
  
  #set the name of the problem set
  ps.name = "Protests-as-Strategic-Games"
  
  #set required libraries
  libs = c("tidyr", "haven","ggplot2", "dplyr", "modelsummary","kableExtra", "glueformula", "estimatr", "gridExtra")
  
  #create problem set
  create.ps(sol.file = paste0(ps.name,"_sol.Rmd"), ps.name=ps.name, user.name = NULL, libs=libs, stop.when.finished=FALSE, addons = "quiz")
  
  #open problem set in web browser
  show.ps(ps.name,launch.browser=TRUE, auto.save.code=FALSE, sample.solution=FALSE)
  
}
run()
```
#>

Welcome to this RTutor problem set! In the following interactive Analysis we are going to explore and reproduce empirical evidence
to better understand the strategic nature of protests and the underlying motivations driving participants.
This problem set is based on the academic paper **"Protests as Strategic Games: Experimental Evidence from Hong Kong's Antiauthoritarian Movement"** by Davide Cantoni, David Y. Yang, Noam Yuchtman and Y. Jane Zhang, which was published in the Quarterly Journal of Economics
in 2019. The article can be found [here](https://academic.oup.com/qje/article/134/2/1021/5298503#133113512). 

**Note:** Please open the links in this problem set in a separate tab, as you might lose the progress you have made in the problem set up until that point.

The decision to protest has long been perceived as a strategic game, with an individuals motivation being dependent on their beliefs regarding others attendance. The question is, whether higher beliefs on turnout increase or decrease participation when looking at antiauthoritarian movements in particular. There is a solid case to be made for both assumptions, as we might be dealing with a classic  collective action problem, with some **free-riding** and profiting from other peoples costly participation. On the other hand, the larger the protest the higher the chances of achieving the movements goals. Moreover, the more people on the streets, the smaller the chances of government intervention and personal sanctions. Thus, other people's attendance could also motivate others to join them.

In order to find out what characterizes the decision to protest, we are going to be taking a closer look at an ongoing, high stakes anti-authoritarian movement, Hong Kong's struggle for democratic reform. Since Hong Kong's handover to the Peoples Republic of China on the first of July 1997, protests have occasionally flared up in the city, demanding civil liberties and democratic reforms. This interactive analysis will guide you through the experiment conducted in the paper **"Protests as Strategic Games: Experimental Evidence from Hong Kong's Antiauthoritarian Movement"**, recalibrating students beliefs on others participation, explaining the methodology the researchers used along the way.

<br/>

## Exercise Contents


1. Political Context

2. Motivation & Experimental Design

3. Data Preparation

4. How does truthful information affect beliefs regarding actual turnout?

5. How does truthful information affect actual turnout?

6. How do beliefs affect turnout?

7. Conclusion

8. References

9. Appendix

<br/>

## How to work on the problem set

When working on this problem set, you will encounter two types of exercises, quizzes and code chunks. The quiz part is pretty self explanatory, though there will always only be one correct answer, which you have to chose.

Code chunks come in three different forms. In some exercises you might have to fill in gaps in the code like this `___`, while in others you might have to write the code yourself. You will also encounter code chunks that are ready to run, as the correct code is already given.
If you want to work on a code chunk press the `edit` button. In case you need help at some point, you can click on the `hint` button, and if bad comes to worse, there is also a possibility of displaying the solution (`solution`). `run` will execute the code in the chunk, and finally, if you want to verify and complete the task, simply click `check`.

If you are finished with the exercise and want to move on, click on `Go to next exercise...` at the bottom of the page.


## Exercise 1. Political Context

<br/>

Before we dive into the experiment, let us set the stage by taking a look at the history and characteristics of Hong Kong's antiauthoritarian movement and the July 1, 1997, protests in particular.


### Brief history of Hong Kong's antiauthoritarian movement

The roots of Hong Kong's antiauthoritarian movement lie in the region's historical and political developments. Hong Kong, a former British colony, was restored to the People's Republic of China's sovereignty on July 1, 1997, under the principle of "one country, two systems" which granted it the status of a special administrative region.

#< info "Special Administrative Region"
Hong Kong, alongside Macao, is one of China's two special administrative regions. Their status grants them a high level of autonomy compared to mainland China. While still directly under the authority of the Peoples Republic of China (PRC), these semi-autonomous regions retain their free market economy, their own currency, their own judiciary, a political system with democratic elements and a quasi constitution (Basic Law). This allows for much greater civil liberties, for instance freedom of expression or assembly and freedom of press. (Federal Foreign Office of Germany, 2020)
#>

#< quiz "Special Administrative Region"
question: Hong Kong is set to keep this status...
sc:
- indefinitely
- until 2037
- until 2047*
success: Great, your answer is correct!
failure: Try again.
#>

In the first years after handover, Hong Kong enjoyed relative political stability and economic prosperity.
However, concerns about the Chinese government's increasing influence and erosion of civil liberties grew, when the National Security Bill was proposed by the Hong Kong Chief Executive in 2002 (Cantoni et al., 2019). The Chief Executive acts as the head of government and is appointed by the Chinese State Council, which is widely seen as pro-Beijing. The National Security Bill specifically sought to implement Article 23 in a bid to regulate speech and behaviour that threatened the government (Amy Gunia, 2019).
On the 6th anniversary of the handover, an estimated half a million people took to the streets to protest against the proposed bill, which lead to its subsequent withdrawal (Cantoni et al., 2019).

<br/>

<center>

![July 1. 2003 protest](images/2003.webp)

Protest against Article 23 on July 1, 2003 - Source: Vincent Yu (AP)

</center>

<br/>

The antiauthoritarian movement also gained momentum during the 2014 Umbrella Movement. In the wake of the Chinese government's decision to tightly control elections for Hong Kong's leaders, protesters were calling for true universal suffrage. 
The Umbrella Movement lasted 79 days and led to mass sit-ins and civil disobedience. Students and protesters were using umbrellas as a means to protect themselves from pepper spray, hence the name of the movement. Although the protests received international attention and broad support, they failed to achieve their immediate political goals (Francis L. F. Lee et al., 2019).

<br/>

<center>

![Umbrella Revolution](images/umbrella_revolution.webp)

A protester in the occupied areas outside government headquarters on Oct. 9, 2014 - Source: Kin Cheung (AP)

</center>


<br/>

Since the Umbrella Movement, Hong Kong's democratic movement has fragmented and the Hong Kong government has intensified its efforts against antiauthoritarian campaigns, banning the Hong Kong National Party for instance.

Most recently, further encroachments on Hong Kong's civil liberties and fear of prosecution among citizens have culminated in the so called "Summer of Uprising" in 2019. The government unveiled an amendment, which would allow extradition from Hong Kong to Mainland China. Inspite initially aiming for the mere withdrawal of the amendment, the protests quickly evolved into a city-wide campaign seeking political reforms, sparking clashes between the police and protesters (Francis L. F. Lee et al., 2019).

#< quiz "Summer of Uprising"
question: How many of Hong Kong's 7,4 million inhabitants participated in the largest protest of 2019?
sc:
- 2 million*
- 700 thousand
- 1,2 million
- 500 thousand
success: Great, your answer is correct! According to the organizers 2 million people participated in the protest on the 16th of June 2019.
failure: Try again.
#>

<br/>

### The July 1st marches and its characteristics

The July 1st marches form the backbone of Hong Kong's ongoing democratic struggle. They have been an annual occurrence on the anniversary of Hong Kong's handover to the Peoples Republic of China in 1997, demanding civil liberties and democracy.

Besides the struggle for policy concessions and civil liberties, the handover anniversary protests share many typical characteristics with other antiauthoritarian protests. They have typically drawn large crowds, despite the threat of government intervention. Depending on the prevailing political climate in the respective year, participation has been ranging between several thousand to around 500 thousand participants (Cantoni et al. 2019). Participants come from various segments of society, including citizens from different age groups and students.
The marches and public gatherings have largely remained peaceful, thus have been tolerated by the authorities.

In 2020, the government intervened and prohibited the anniversary protests in the wake of the COVID-19 pandemic, applications to hold the march were rejected. The July 1st marches have not been taking place ever since. On top of denied applications, many important pro-democracy groups, most importantly the Civil Human Rights Front have disbanded during the pandemic. The Civil Human Rights Front was responsible for some of Hong Kong's biggest antiauthoritarian protests, and organized the yearly July 1st protests (CNN, 2021).

With the government and police force rejecting protest applications, tightening rules and regulations and with the most important organizers gone, it is in question, whether July 1st marches will be held in the future, and if the movement can sustain itself longterm.

## Exercise 2. Motivation & experimental design

<br/>

### Motivation

When mobilizing people to attend the July 1st protests, political entities in Hong Kong have essentially been using two different strategies to advertise. The first strategy is to paint a positive picture, forecasting high turnout, advertising it as a fun time, as many people are expected to attend. The second approach that the other political groups use to mobilize protesters, is to paint a negative picture, stating concern that not as many people will attend, and that Hong Kong citizens should feel obligated to protest against the government (David Yang, 2019).

But which one is more effective? What drives individuals' decisions to take part in antiauthoritarian protests?

For decades now, the decision to protest has been seen as strategic among economists, which means, that an individuals participation is dependent on other people's attendance. Protests often exemplify a classic case of a political collective action problem, known as **free-riding**. Since a certain threshold of participation creates a public good, individuals might have an incentive to stay at home, avoiding possible sanctions by the government, profiting from other individuals costly participation. The more people participate, the less individuals might be willing to attend themselves, thus producing a game of **strategic substitutes** (Palfrey and Rosenthal, 1984). 
In theory, this would not just render the first strategy, i.e. to persuade individuals to join the protest by forecasting high turnout, useless, but would be also counterproductive by discouraging turnout among individuals as well. 

However, recent theoretical work suggests a game of **strategic complements**. This means, that individuals might be persuaded by other peoples attendance to join themselves. The main reasoning behind this theory is, that the perceived cost of participation decreases, if more people participate: The more people go out to the streets, the harder it is going to be for the authorities to crack down on protests, and the likelihood of individual sanctions diminishes. On top of that, a larger crowd usually increases the chances of achieving the protests goals such as democratic reforms (Bueno de Mesquita, 2010). According to Gehlbach, Sonin, and Svolik (2016) **strategic complementarity** even "characterizes mass protests". 
In a setting of **strategic complements** the first (more positive) strategy would be more effective. In contrast, the second strategy, stating that not enough people are projected to participate, might actually discourage participation, as the perceived cost of going increases, when less people are present. 

To find out whether the choice of attending a protest is a **strategic game of substitutes** or **complements**, we are looking at an ongoing high stakes antiauthoritarian movement.  The authors of the paper **"Protests as Strategic Games: Experimental Evidence from Hong Kong's Antiauthoritarian Movement"**, Davide Cantoni, David Y. Yang, Noam Yuchtman and Y. Jane Zhang, conducted a framed field experiment at the Hong Kong University of Science and Technology, monitoring students before and after the July 1st protests. 

Survey results from this experiment on student's perception of cost and benefit show the fascinating nature of strategic protest decision.

**Task:** Check the chunk below.

```{r}
#< task_notest
library(dplyr)
library(ggplot2)
dat<-readRDS("data/dat.RDS")

var_list <- c("prot_crackdown_10000_w3", "prot_crackdown_50000_w3","prot_crackdown_250000_w3", "prot_crackdown_1250000_w3", "prot_democracy_10000_w3", "prot_democracy_50000_w3", "prot_democracy_250000_w3", "prot_democracy_1250000_w3")

fig_1 <- dat%>%
  filter(hk_local == 1)%>%
  select(var_list)%>%
  summarise_all(mean, .groups = "rowwise")%>%
  pivot_longer(cols = var_list)%>%
  mutate(type = ifelse(substring(name,6,8) == "cra","Govt crackdown", "Democracy achieved in HK"),
         turnout = case_when(
           str_detect(name,"1250000") ~ "1250000",
           str_detect(name,"250000") ~ "250000",
           str_detect(name,"50000") ~ "50000",
           str_detect(name,"10000") ~ "10000"),
         value = value*10)

fig_1%>%
  ggplot(aes(x = factor(turnout, levels = c("10000","50000","250000","1250000")), y = value, colour = type, group = type, linetype = type, shape = type)) +
    geom_point(aes(size = type)) + 
    geom_line(size = 0.9) +
    labs(x = "Hypothetical protest turnout",
         y = "Belief on probability of ... if # turnout to protest",
         title = "Figure 1: Students beliefs on cost and benefit of protests") +
    theme_classic() + 
    scale_y_continuous(breaks = c(10,20,30,40,50,60)) +
    scale_shape_manual(values = c(16,18)) +
    scale_color_manual(values = c("#000066", "#990000")) +
    scale_linetype_manual(values = c("longdash", "solid")) +
    scale_size_manual(values = c(2,2.7)) +
    theme(legend.position = "bottom",
          legend.title = element_blank())
#>
```

#< quiz "Cost and benefits of protests"
question: Students beliefs regarding the benefits and costs of hypothetical protests point towards...
sc:
- strategic substitutability
- strategic complementarity
- both strategic substitutability and complementarity*
- neither strategic substitutability nor complementarity
success: Great, your answer is correct!
failure: Try again.
#>

Interestingly, we observe influences of both **strategic substitutability** and **complementarity** among students in Hong Kong.
Their perception of success in achieving democracy with higher hypothetical turnout points towards **strategic complementarity**, while growing fear of government intervention contradicts its fundamental assumptions. At this point, individuals have an incentive to **free-ride** indicating **strategic substitutability** instead.

In order to answer the question at hand, we are going to identify the causal effects of an individual's belief regarding other peoples turnout on their own participation step by step with the help of the experiment conducted in the underlying paper. 

### Experimental design

The framed field experiment was conducted as an online survey in three parts. Participants were recruited via email from the entire undergraduate student population at the Hong Kong University of Science and Technology (HKUST). The experimental sample was drawn from the 1,741 students (19% response rate) who completed the initial survey (Cantoni et. al.). Students were paid 25$ for each survey and received additional payment as a function of choice in the incentivized games.

The experiment is designed to isolate the causal effect of variation in beliefs regarding other subjects turnout on one's own participation, to find out whether higher projected turnout increases or decreases participation. In order to achieve that, a random subset of the experimental sample were provided with truthful information intended to shift their beliefs (Cantoni et. al.). 

<center>

![Experimental Design](images/Experimental_Design.jpeg){width=80%, height = 80%}

Experimental design - Source: Own illustration based on Cantoni et. al.

</center>

<br/>

In the first part of the experiment on June 24th, the student's own planned participation was elicited as part of the first survey in both control and treatment group. On top of the question, whether the individuals themselves were planning to attend, subjects were asked to provide a guess on planned and actual participation, regarding the upcoming July 1st march for HKUST students and Hong Kong's population respectively. Additionally, a broad range of characteristics, for instance political attitudes and personality traits, were also elicited.

Part two of of our experiment is where the intervention takes place. On the 30th of June, two thirds of the experimental sample were randomly assigned to the treatment group and received informational treatment. Subjects, who were assigned to the treatment group, were reminded of their prior beliefs on planned participation and received truthful information on the sample average of planned participation (roughly 17%).
Both control and treatment group then had the chance to change their guess on actual participation among students and the total turnout regarding the protest taking place the following day.

As mentioned above, beliefs in both stage one and two were elicited in an incentivized manner: If the guess of a subject was within a two percentage point range from the truth, they received an extra 10$, in order to encourage sincere responses to the survey.

In the the third and final part of the experiment, students had to report on their participation after the July 1st protest.

In order to estimate the causal effects of beliefs on actual participation, we are going to proceed as follows:
First, we are going to look at the **first stage**, between part one and part two of the experiment, in order to observe how truthful information shifts beliefs among students. Afterwards, we are going to estimate the **reduced form**, the direct effect of truthful information on actual participation in part three. Last but not least, we are going to estimate the causal effect of beliefs after the intervention in part two on participation, combining **first stage** and **reduced form**.  



## Exercise 3. Data preparation

<br/>

Before we begin with the analysis we need to prepare our data. The data set *protests_strategic_games.dta* is stored in the `.dta` file format, which is commonly used by the statistical software Stata. As base R does not have the capabilities to read such files, we need the help of the `haven` package. To load a package, simply call the library command with the name of the desired package in the parentheses.

**Task:** Start by loading the `haven` package using `library()`.

```{r}

#< task
#use library() to load the haven package
#>
library(haven)

```

`haven` allows us to load foreign data formats such as Stata files in to R. In this specific instance we are going to be using the *read_dta* function to read the DTA file.

**Task:** Use the `read_dta` function from the `haven` package to load the dataset and store it in the variable `data`.

```{r}
#< fill_in
#Fill in the blanks
data<-___("data/protests_as_strategic_games.dta")
#>
data<-read_dta("data/protests_as_strategic_games.dta")
```

Now that we have successfully read the file, let us take a brief look at data. A quick and easy way of gaining an overview is to use the `head()` function in base R. 
The `head()` function by default displays the first six rows of the respective dataframe, you can however change the number of rows to be displayed inside the statement.

**Task:** Display the first 10 observations of the dataset.

```{r}
#< task

#>
head(data, 10)
```

<br/>

At first glance, some of the variable names seem to be a bit inconvenient so we are going to rename the most important ones. The `rename()` function from the `dplyr` package offers an easy way to do this. One column in the dataset we are going to be using a lot is `belief_treatment_w3`. It is the dummy variable, identifying students as part of the treatment group in the experiment. Simplifying these variable names might seem trivial now, however, it's going to make life much easier further down the line. 

**Task:** Check the following chunk to load the `dplyr` package.

```{r}
#< task_notest
library(dplyr)
#>
```


**Task:** Fill in the blanks to change the variable name `belief_treatment_w3` to `treat`.

```{r}
#< fill_in
dat <- data%>%
  rename(___ = ___)

#>
dat <- data%>%
  rename(treat = belief_treatment_w3)

```

<br/>

You might have noticed the usage of `%>%` in the previous task. The so called *pipe* operator is part of the `dplyr` package, and allows us to create a chain of functions. 
*Pipes* take the output of the previous function and feed it to the next one. In this example, we fed our dataset to the `rename()` function.

#< info dplyr

*Pipechains* can be understood as a sequence of actions applied to an object one after another. 

In addition to *pipes*, `dplyr` offers great functions for structured data manipulation:  

- `mutate()`: Adds new variables as function of existing variables 
- `select()`: Selects variables from the dataset based on their name
- `filter()`: Selects observations based on condition
- `summarise()`: Aggregates data based on groups passed by `group_by()`
- `arrange()`: Rearranges the order of observations

The `group_by`function is most commonly used in combination with `summarise()`, however it naturally combines with the other functions as well.
Keep in mind that the `dplyr` functions do not directly alter the initial input. If the changes are ought to be permanent, the resulting dataframe has to be assigned to a variable.

You can find a comprehensive overview over dplyr's functions [here](https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf).

#>

As part of the analysis we need to create a dummy variable `birth_place_hk`, which is 1, if the student was born in Hong Kong ,and 0 otherwise. The dataset does include the variable `birth_place`, which we are going to be using. When adding new variables as a function of existing ones, we can put the *pipe* operator and `mutate()` to good use as demonstrated in the example below:

**Task:** Check the following chunk.

```{r}
#< task_notest
data <- dat%>%
  mutate(birth_place_hk = ifelse(birth_place == "HK", 1, 0))
#>
```

Now we want to create another dummy similar to the example above. This variable is supposed to be 1, if the student was either born in Hong Kong (`birth_place_hk` == 1) or moved there at the age of 10 or earlier (`move_hk_age` <= 10). 

**Task:** Create the dummy as described above using `dplyr` grammar and assign it to `hk_local` and save it in `data`.

```{r}
#< task

#>
data <- data%>%
  mutate(hk_local = ifelse(birth_place_hk == 1 | move_hk_age <= 10, 1, 0))
```

The experiment is going to focus Hong Kong natives. This means that we need to get rid of international students and other groups of subjects who were not born in Hong Kong or moved there late. We can easily identify them using the dummy above. On top of that, we need to discard observations, that do not have a treatment status (NA).

**Task:** Fill in the blanks to filter the data set as described above and save it in `dat`.

```{r}
#< fill_in
#Fill in the blanks
___<-data%>%
  ___(hk_local == 1, !___(treat))
#>

dat<-data%>%
  filter(hk_local == 1, !is.na(treat))

```


### Balance check and internal validity

The authors of the original paper conducted a **randomized experiment**, which is the scientific gold standard to estimate causal effects. Randomly assigning treatment status to subjects independent of individual characteristics solves many problems regarding internal validity (Rieber, 2021). However, in order to estimate unbiased causal relationships in a **randomized experiment** the independent variables have to be balanced between the control and treatment group. If individual characteristics were different in either of the two groups, we might not be able to isolate the causal effect of treatment on the dependent variable, as we face endogeneity problems.
Thus, before we jump into the main part of the analysis, we need to make sure that characteristics in both control and treatment group are similar and not significantly different in our experimental sample.

The experimental sample consists of students whose posterior beliefs were elicited on June 30th and who answered the followup questions provided after the July 1st protest.

**Task:** Check the chunk to filter the experimental sample.

```{r}
#< task_notest
dat <- dat%>%
  filter(followup_postjuly1st_w3 == 1, !is.na(posterior_belief_part_hkust))
#>
```

We want to create a table displaying the means of beliefs and characteristics for each variable, split between control and treatment.

**Task:** Fill in the blanks to calculate the means of every variable stored in `varlist`for the control and treatment group respectively.

```{r}
#< fill_in
varlist <- c("planned_part", "belief_planned_part_hkust", "prior_belief_part_hkust", "prior_belief_part_total", "gender", "birth_year", "az_demogra_childenv", "religion_none", "az_demogra_hhecon", "az_demogra_ownecon", "monthly_income_total_w3", "realest_totalowned_w3", "father_educ_hsabove_w123", "mother_educ_hsabove_w123", "hs_language_english", "hkgeneration_w123")

tab <- dat%>%
  ___(treat,varlist)%>%
  group_by(___)%>%
  ___(round(across(where(is.numeric), mean, na.rm = TRUE),3))

#>
varlist <- c("planned_part", "belief_planned_part_hkust", "prior_belief_part_hkust", "prior_belief_part_total", "gender", "birth_year", "az_demogra_childenv", "religion_none", "az_demogra_hhecon", "az_demogra_ownecon", "monthly_income_total_w3", "realest_totalowned_w3", "father_educ_hsabove_w123", "mother_educ_hsabove_w123", "hs_language_english", "hkgeneration_w123")

tab <- dat%>%
  select(treat,varlist)%>%
  group_by(treat)%>%
  summarise(round(across(where(is.numeric), mean, na.rm = TRUE),3))
```

In addition to the means we calculated above, we perform a two sample t-test to examine if the means are equal in both groups. Subsequently, we combine both the means and the p-values resulting from the t-test in a table using `kableExtra`.

**Task:** Check the chunk to add p-values and to create the table.

```{r}
#< task_notest
p.value <- 0

for (var in varlist) {
p.value[var] <- round(t.test(dat[[var]][dat$treat == 0],
                  dat[[var]][dat$treat == 1])$p.value,3)

}

varnames <- c("Planned to participate in protest", "Prior belief re: others planned participation (%)", "Prior belief re: others actual participation at HKUST (%)", "Prior belief re: others actual participation in total (%)", "Male", "Birth Year", "Hk-oriented childhood", "Nonreligious", "HH economic and social status", "Own projected economic status", "Monthly family income", "Properties parents own", "Fathers educational attainment above high school", "Mothers educational attainment above high schhol", "High school language was english", "Generations since family migrated to HK")

table <- tab%>%
  pivot_longer(!treat, names_to = "variable", values_to = "value")%>%
  pivot_wider(names_from = treat, values_from = value)


table <- cbind("Variables" = varnames, table, "p.value" = unname(p.value[-1]))[,-2]

library(kableExtra)

table%>%
  kbl(col.names = c("Variables", "Control", "Treatment", "p-value"), align = "l", caption = "Table 1: Summary Statistics and Balance Check")%>%
  kable_styling("hover")%>%
  add_header_above(c(" " = 1, "Participated in 2016 July 1 march" = 3))%>%
  pack_rows(index = c("Protest related characterisics" = 4, "Personal Characteristics" = 12))
#>
```

#< quiz "Balance Check"
question: Are the treatment and control group balanced?
sc:
- yes*
- no
success: Great, your answer is correct!
failure: Try again.
#>

Table 1 definitely indicates good randomization as we can't observe significant differences between the means of the control and treatment group. If we look at the p-values resulting from the t-tests, we cannot reject the null hypothesis (equality of means) for any of our variables either, as they are not lower than at least 0.1, indicating that treatment was indeed well randomized.

Besides proper randomization, one of the biggest concerns of internal validity researchers face when conducting experiments involving an authoritarian regime is, whether subjects are afraid to answer questions from the survey truthfully. This is especially relevant, when it comes to self reported participation, as subjects might be afraid to face sanctions. According to the authors of the underlying paper, this is fairly unlikely, though. If students are willing to participate in a public protest, it would not make sense for them to shy away from truthful answers in a private survey (Cantoni et al., 2019).

In the following, we test this nonetheless, to ensure internal validity moving forward.

In the first part of the study, the authors elicited several key dimensions of political ideology that subjects might not want to answer truthfully, as they might be considered sensitive using **list experiments** or the **item count technique** (Cantoni et al., 2019).
Using the item count technique, we take a look whether students respond truthfully on whether they have an unfavorable view of the Chinese Communist Party (CCP), they consider themselves Hong Kongese, whether they support HK independence and if they support the use of violence in pursuit of HK's political rights. 

#< info "Item count technique"
The item count technique is a method to test whether a subjects answer to a sensitive question changes, when cover is provided. The authors of the paper adopt a modified version of the standard list experiment based on Coffman, Coffman and Ericson (2017): 

First step is to split the sample in two groups, direct report treatment (`list_exp_direct_w3`) and the veiled report treatment (`list_exp_veiled`). Subjects in both groups were asked 5 questions, one of them being on sensitive subjects and 4 innocuous questions.

```{r eval = FALSE}
data<-data%>%
  mutate(list_exp_veiled = 1 - list_exp_direct_w3)
```

In the direct report treatment group subjects had to answer the sensitive question directly with no cover provided, for example whether they support independence. If they answered with "yes", the corresponding dummy `list_dummy_indp_w3` would be 1 and 0 otherwise. `list_count_indp_direct_w3` is the number of the 4 innocuous questions that subjects answered with "yes". If we add them together, we receive the overall number of questions answered with "yes" out of five, including the direct question. 

```{r eval = FALSE}
data<-data%>%
  mutate(list_countd_indp_direct = list_count_indp_direct_w3 + list_dummy_indp_w3,
         list_countd_hkid_direct = list_count_hkid_direct_w3 + list_dummy_hkid_w3,
         list_countd_fccp_direct = list_count_fccp_direct_w3 + list_dummy_fccp_w3,
         list_countd_viol_direct = list_count_viol_direct_w3 + list_dummy_viol_w3)
```

In the the veiled report treatment group we do not observe the subjects answer to the sensitive question, but just the number of questions answered with "yes" out of all five in `list_count_indp_veiled_w3`. As a result, we cannot tell, how the subject answered the sensitive question in this group, as the four innocuous questions provide cover. Finally we merge both groups together into one variable `list_count_indp`, containing the number of true items. This way we can estimate the effect of veiled questions on the list count in a linear regression.

```{r eval = FALSE}
data<-data%>%
  mutate(list_count_indp = ifelse(list_exp_direct_w3 == 1, list_countd_indp_direct, list_count_indp_veiled_w3),
         list_count_hkid = ifelse(list_exp_direct_w3 == 1, list_countd_hkid_direct, list_count_hkid_veiled_w3),
         list_count_fccp = ifelse(list_exp_direct_w3 == 1, list_countd_fccp_direct, list_count_fccp_veiled_w3),
         list_count_viol = ifelse(list_exp_direct_w3 == 1, list_countd_viol_direct, list_count_viol_veiled_w3))
```

The underlying assumption of this method is that the expected number of true items would be the same in both groups under truthful reporting. If they differ, the veiled group offers a more accurate estimate of the mean, as veiled report treatment reduced the cost of answering truthfully (Coffman, Coffman and Ericson, 2017). 
#>

Let us take a look at the direct answers to the four sensitive questions.

**Task:** Check the chunk to calculate the means of the direct questions.

```{r}
#< task_notest
tb <- data%>%
  filter(list_exp_direct_w3 == 1, hk_local == 1)%>%
  summarise(ufccp = round(1 - mean(list_dummy_fccp_w3, na.rm = TRUE),3),
            hkid = round(mean(list_dummy_hkid_w3, na.rm = TRUE), 3),
            indp = round(mean(list_dummy_indp_w3, na.rm = TRUE), 3),
            viol = round(mean(list_dummy_viol_w3, na.rm = TRUE), 3))
tb
#>
```

92.3% seem to have an unfavorable view on the CCP, a large majority considers themselves as Hong Kongese and less than half of subjects answered in support of independence. Most importantly 21.6% support violence in pursuit of Hong Kong's political rights.

The **item count technique** now allows us to estimate how the answers above change if cover is provided.

#< quiz "Item count technique"
question: Which attitude is most likely to change when cover is provided?
sc:
- Unfavorable view of CCP
- Consider self Hong Kongese
- Support HK independence
- Support violence in pursuit of policital rights*
- none of the above
success: Great, your answer is correct!
failure: Try again.
#>

**Task:** Check the chunk below to show the results of the item count technique.

```{r}
#< task_notest
tab <- data%>%
  filter(hk_local == 1)

reg1<-lm(-list_count_fccp ~ list_exp_veiled, data = tab)
reg2<-lm(list_count_hkid ~ list_exp_veiled, data = tab)
reg3<-lm(list_count_indp ~ list_exp_veiled, data = tab)
reg4<-lm(list_count_viol ~ list_exp_veiled, data = tab)

tb_2 <- pivot_longer(tb, cols = c(ufccp, hkid, indp, viol))
tb_2 <- as.data.frame(cbind(Attitudes = c("Unfavorable view of CCP", "Consider self Hong Kongese", "Support for HK independence", "Support for violence in pursuit of HK's political rights"), "'yes' in direct question" = tb_2$value))
attr(tb_2, "position") <- c(1,2)

modelsummary(list(reg1,reg2,reg3,reg4),
             output = "kableExtra", 
             shape = model ~ term,
             coef_omit = "(Intercept)",
             coef_rename = c("list_exp_veiled" = "Î” When cover provided"),
             statistic = NULL,
             estimate = "{estimate}{stars} [{std.error}]",
             add_columns = tb_2,
             title = "Table 2: Item count experiments and willingness to respond to direct questions")
#>
```

As we can see there is no significant change for three of our four sensitive questions, when cover is provided. Even for a more extreme stance like supporting independence, subjects do not shy away from answering truthfully in the direct questions. When it comes to the question, whether subjects support the use of violence in pursuit of political rights, there is a significant difference when cover is provided. This indicates that subjects use the cover provided if needed, though they do not need it when responding to political questions regarding nonviolent opposition. 

Ultimately if a student is willing to answer questions about their support of independence and opposition to the CCP freely in direct questions, they would not be inclined to lie in a private survey. 


In the following exercise we are going to start with the main part of our analysis, as we are going to be looking at the treatment effect on posterior beliefs. Three variables we are going to be using a lot throughout the analysis are `belief_planned_part_hkust`, `prior_belief_part_hkust` and `posterior_belief_part_hkust`, so it is important we can tell them apart. `belief_planned_part_hkust` and 
`prior_belief_part_hkust` were both elicited in stage one of the experiment, with the former being the subjects guess on planned participation and the latter being the belief regarding actual participation. `posterior_belief_part_hkust` contain the beliefs on actual participation elicited after the intervention in part two. 


## Exercise 4. How does truthful information affect beliefs regarding actual turnout?

<br/>

In the first stage of the experiment, we want to estimate, how truthful information affects beliefs on actual turnout.

Before we start, we need to load the data set again. From now on, we will be using a modified version of the original data set with the changes from the previous exercise already incorporated. This will save us a lot of time, because we do not have to perform the data preparation for every exercise individually.

The prepared data set is stored as an *.RDS* file, which means we are going to use the `base` *R* function readRDS() to access it.

**Task:** Use `readRDS()` to load `dat.RDS` from the data folder and save it in `data`

```{r}
#< task

#>

data<-readRDS("data/dat.RDS")
```

## Descriptive statistics

In the first stage of the experiment, we are examining the information treatment effects on posterior beliefs, regarding actual turnout. In other words, the recalibration of beliefs, induced by the intervention. Subjects who were part of the treatment group were reminded of their guess in the first survey (part one of the experiment) of how many subjects planned to attend the July 1st march, and were informed, that 17% of experimental subjects planned to participate.
Both control and treatment group were then asked to provide another guess the day before the protest in part two.

We can visualize this information effect by looking at the density of beliefs of HKUST students before and after the intervention. 

To do that, we first need to make sure, that we have obtained data before and after the treatment for all observations. The dummy variable `followup_postjuly1st_w3` indicates whether the subjects beliefs were elicited in the second survey.

**Task:** Restrict the sample to observations where a followup was elicited.

```{r}
#< fill_in
dat<-___%>%
  filter(followup_postjuly1st_w3 == 1)
#>
dat<-data%>%
  filter(followup_postjuly1st_w3 == 1)
```

Now we can plot the the density of beliefs prior to the intervention within the experimental sample using the `geom_density()` function from the `ggplot2` package.

#< info ggplot2
`ggplot2`  is a data visualization package for R that enables users to create multi-layered graphics.

`ggplot2` illustrations are structured in a modular system. the `ggplot()` function creates an object and allows the user to specify basic parameters like the data, that is to be used in the plot. Subsequently multiple different layers of `geoms`(visual marks that represent datapoints) can be added to the object. In this problem set we are going to be using the following ones: 

- `geom_point()`: Creates a scatterplot in a new layer.
- `geom_line()`: Creates a linegraph in a new layer.
- `geom_vline()`: Creates a vertical line in a new layer.
- `geom_bar()`: Creates a barchart in a new layer.
- `geom_density()`: Creates a plot layer of a variables density.
- `geom_errorbar()`: Creates a plot layer with vertical intervals for each observation.

The aesthetics mapping (`aes()`) allows the user to specify the x and y axis of the plot, as well as features like size and color of data points (or lines). 

You can find a comprehensive overview over `ggplot` and its use [here](https://github.com/rstudio/cheatsheets/blob/main/data-visualization.pdf).

#>

For an example on how to use the function and `ggplot` in general, check the chunk below.

**Task:** Click `check` to load the `ggplot2` package and plot the density of `prior_belief_part_hkust`.

```{r}
#< task_notest
library(ggplot2)

ggplot()+
  geom_density(data = dat, aes(x = prior_belief_part_hkust, colour = "Prior to follow-up"), size = 1)+
  geom_vline(xintercept = 17, linetype = "dashed")+
  scale_color_manual(values = "#666666")+
  theme_classic()+
  labs(x = "Beliefs re: actual participation (among HKUST)",
       y = "Density")+
  theme(legend.position = "bottom",
        legend.title = element_blank())
#>
```

The plot shows the distribution of HKUST student's prior beliefs regarding other student's actual participation, the vertical line indicates the true level of planned participation. As we can see, expectation prior to the intervention for other subjects planned participation is mostly distributed between 0 and 10%. Most students don't seem to expect 17% or more of their fellow students to be planning to participate.

In order to observe the information treatment effect, we now need to compare the subject's beliefs in control and treatment group, after the latter received truthful information of the planned participation of 17% among students. Let's add the density of both groups posterior beliefs to the plot above.

**Task:** Fill in the blanks to add the density of `posterior_belief_part_hkust` for control and treatment group respectively.

```{r}
#< fill_in
ggplot()+
  geom_density(data = dat, ___(x = prior_belief_part_hkust, colour = "Prior to follow-up"), size = 1)+
  geom_density(data = filter(dat, treat == 0), aes(x = ___, colour = "Posterior to follow-up (control)"), size = 1)+
  geom_density(data = filter(dat, treat == 1), aes(x = ___, colour = "Posterior to follow-up (treatment)"), size = 1)+ 
  geom_vline(xintercept = 17, linetype = "dashed")+
  labs(x = "Beliefs re: actual participation (among HKUST)",
       y = "Density",
       title = "Figure 2: Distribution of Prior and Posterior Beliefs among Students") +
  theme_classic()+
  scale_color_manual(values = c("#000000", "#990000", "#666666"))+
  theme(legend.position = "bottom",
          legend.title = element_blank())
#>

ggplot()+
  geom_density(data = dat, aes(x = prior_belief_part_hkust, colour = "Prior to follow-up"), size = 1)+
  geom_density(data = filter(dat, treat == 0), aes(x = posterior_belief_part_hkust, colour = "Posterior to follow-up (control)"), size = 1)+
  geom_density(data = filter(dat, treat == 1), aes(x = posterior_belief_part_hkust, colour = "Posterior to follow-up (treatment)"), size = 1)+ 
  geom_vline(xintercept = 17, linetype = "dashed")+
  labs(x = "Beliefs re: actual participation (among HKUST)",
       y = "Density",
       title = "Figure 2: Distribution of Prior and Posterior Beliefs among Students") +
  theme_classic()+
  scale_color_manual(values = c("#000000", "#990000", "#666666"))+
  theme(legend.position = "bottom",
          legend.title = element_blank())
```

#< quiz "Density of beliefs"
question: Subjects in the treatment group...
sc:
- all updated their beliefs positively
- updated their belief towards the true level of planned participation*
- consistently updated their beliefs negatively
success: Great, your answer is correct!
failure: Try again.
#>

The beliefs of subjects from the control group follow a similar trend as the beliefs prior to the intervention. However, the treatment group's posterior beliefs are distributed much more tightly between 10% and 20%. Students from the treatment group, who trust the information provided, seem to have updated their beliefs towards the 17% truth, hence the more compressed distribution. Even though at first glance, it looks like beliefs increased overall, we cannot observe a positive effect when looking at the density. This would not make sense anyway, since subjects who overestimate planned participation will likely also change their beliefs towards the true 17%.

If we apply this logic, lower priors are expected to update their beliefs positively and higher priors negatively. We can examine the updating of beliefs among students more closely by looking into the change of beliefs for each individual observation. First, we need to create a variable called `belief_change` which is calculated by posterior (`posterior_belief_part_hkust`) minus prior (`prior_belief_part_hkust`) beliefs.

**Task:** Create the variable `belief_change` in `data` using `dplyr` grammar.

```{r}
#< fill_in
data <- data%>%
  ___(belief_change = ___)

head(data[, c("belief_planned_part_hkust", "belief_change")], n = 5)
#>
data <- data%>%
  mutate(belief_change = posterior_belief_part_hkust - prior_belief_part_hkust)

head(data[, c("belief_planned_part_hkust", "belief_change")], n = 5)
```

Now we can plot the change of belief for each individual from the treatment group against their prior belief on planned participation(`belief_planned_part_hkust`). This way we can observe the treatment effect for different levels of prior beliefs, allowing us to identify whether higher and lower priors react differently to the truthful treatment.

**Task:** Fill in the blanks to create a scatterplot with `belief_planned_part_hkust` on the x-axis and `belief_change` on the y-axis.

```{r}
#< fill_in
ggplot(data = filter(data, treat == 1), aes(x = ___, y = ___))+
  ___()+
  geom_smooth(method = "lm", se = FALSE, colour = "#990000")+
  theme_classic()+
  labs(x = "Prior belief re: % students who plan to participate",
       y = "Change in belief after treatment") 
#>

ggplot(data = filter(data, treat == 1), aes(x = belief_planned_part_hkust, y = belief_change))+
  geom_point()+
  geom_smooth(method = "lm", se = FALSE, colour = "#990000")+
  theme_classic()+
  labs(x = "Prior belief re: % students who plan to participate",
       y = "Change in belief after treatment") 

```

In its current form, the graph looks cluttered. Even though the underlying trends are somewhat observable interpreting it won't yield great results.

To make it a little easier, we can create a **binned scatterplot** instead.

#< info "Binned scatterplot"

The *binned scatterplot* is a variation of the generic scatterplot, that is particularly used to deal with large amounts of data points. *Binned scatterplots* create conditional means in order to reduce the amount of data points in the scatterplot, thus limiting noise in the graph. 

First step when creating a *binned scatterplot* is to divide the conditioning variable on the y-axis into equally sized bins or quantiles.
The next step is to calculate the mean of the dependent variable within each bin, to create one data point per bin. 

#>

There are easy implementations in R that create *binned scatterplots*, like `binscatter()` from the package `binsreg`, however we are going to do it step by step ourselves: 

Let us start by dividing the conditional variable `belief_planned_part_hkust` into 17 quantiles.

**Task:** Fill in the blanks to use the `ntile()` function to create a new column `bin` containing the respective quantile in the `data` table.

```{r}
#< fill_in
data<-data%>%
  mutate(___)

head(data[, c("belief_planned_part_hkust", "bin")], n = 5)
#>
data<-data%>%
  mutate(bin = ntile(belief_planned_part_hkust, 17))

head(data[, c("belief_planned_part_hkust", "bin")], n = 5)
  
```

The next step is to create the binned variables by calculating the belief change for each bin, for control and treatment separately:

**Task:** Fill in the gaps to summarise both `belief_change` and `belief_planned_part_hkust` for each bin.

```{r}
#< fill_in
dat <- data%>%
  group_by(___, ___)%>%
  summarise(belief_change = mean(___, na.rm = TRUE),
            belief_planned_part_hkust = max(belief_planned_part_hkust)-(max(belief_planned_part_hkust) - min(belief_planned_part_hkust))/2)%>%
  mutate(group = ifelse(treat == 1, "Treatment", "Control"))

head(dat[, c("group", "belief_change", "bin")], n = 6)
#>
dat <- data%>%
  group_by(bin, treat)%>%
  summarise(belief_change = mean(belief_change, na.rm = TRUE),
            belief_planned_part_hkust = max(belief_planned_part_hkust)-(max(belief_planned_part_hkust) - min(belief_planned_part_hkust))/2)%>%
  mutate(group = ifelse(treat == 1, "Treatment", "Control"))

head(dat[, c("group", "belief_change", "bin")], n = 6)

```

Now plot the result for the treatment group in a simple scatterplot once again, fitting a line through the observations with `geom_smooth()`. 

**Task:** Fill in the blanks to create the scatterplot.

```{r}
#< fill_in
reg_treat <- lm(belief_change ~ belief_planned_part_hkust, data = filter (data, treat == 1))

ggplot(data = filter(dat, treat == 1), aes(x = belief_planned_part_hkust, y = belief_change))+
  ___()+
  geom_abline(slope = coef(reg_treat)["belief_planned_part_hkust"],
              intercept = coef(reg_treat)["(Intercept)"],
              colour = "#990000",
              lwd = 1.1)+
  ___(method = "lm", se = FALSE, colour = "black")+
  geom_vline(xintercept = 17, linetype = "dashed", color = "gray")+
  ylim(-30,10)+
  xlim(0,80)+
  labs(title = "Treatment group",
       x = "Prior belief: % students plan to participate",
       y = "Change in beliefs after treatment")+
  theme_classic()
#>
reg_treat <- lm(belief_change ~ belief_planned_part_hkust, data = filter (data, treat == 1))

ggplot(data = filter(dat, treat == 1), aes(x = belief_planned_part_hkust, y = belief_change))+
  geom_point()+
  geom_abline(slope = coef(reg_treat)["belief_planned_part_hkust"],
              intercept = coef(reg_treat)["(Intercept)"],
              colour = "#990000",
              lwd = 1.1)+
  geom_smooth(method = "lm", se = FALSE, colour = "black")+
  geom_vline(xintercept = 17, linetype = "dashed", color = "gray")+
  ylim(-30,10)+
  xlim(0,80)+
  labs(title = "Treatment group",
       x = "Prior belief: % students plan to participate",
       y = "Change in beliefs after treatment")+
  theme_classic()
```

The **binned scatterplot** we just created shows the average changes in belief, regarding actual protest turnout among HKUST survey participants, who received informational treatment for each of the 17 quantiles of prior belief on planned participation.
When examining the distribution of the participants beliefs earlier, we already saw a trend towards the true 17%. The **binned scatterplot** now confirms, that we are indeed dealing with **offsetting treatment effects**: This means that the sign of the treatment effect varies within the experimental sample. Subjects from the treatment group whose prior guess was below the actual planned participation consistently updated their beliefs positively. Similarly, students whose prior guess was above the information provided updated their beliefs negatively. If we were to estimate an average treatment effect, the subgroups coefficients might cancel each other out and the treatment effect might be estimated to be 0, even though we observe a significant shift in beliefs above. 

The red line indicates the regression fit on the actual data before binning. This goes to show, that aggregating the data and then fitting the line with `geom_smooth`, might not show the exact effects, as it is prone to outliers. The actual regression line is a bit steeper than the regression line we fit through the bins.

#< quiz "Effect on Beliefs in the Control Group"
question: Do we expect significant updating of beliefs in the control group?
sc:
- yes
- no*
success: Great, your answer is correct!
failure: Try again.
#>

**Task:** Check the chunk below to plot both groups together.

```{r}
#< task_notest
ggplot(data = dat, aes(x = belief_planned_part_hkust, y = belief_change))+
  geom_point()+
  geom_vline(xintercept = 17, linetype = "dashed", color = "gray")+
  geom_smooth(method = "lm", se = FALSE, colour = "black")+
  ylim(-30,10)+
  xlim(0,80)+
  labs(title = "Figure 3: Changes in beliefs regarding actual turnout in treatment \nand control group",
       x = "Prior belief: % students plan to participate",
       y = "Change in beliefs after treatment")+
  theme_bw() +
  facet_wrap(vars(group))
#>
```

**Note:** As the number of bins was not specified in the replication file, the graph slightly differs from the original in the paper.

The decision to protest in broader terms, but also the beliefs on turnout are sequential, which means outward influences as well as individual characteristics might change beliefs over time regardless of treatment.
In the control group however, we usually would not expect significant updating of beliefs towards the information provided. 
Surprisingly, subjects from the control group also updated their beliefs towards the true 17% planned participation. Lower priors updated their beliefs positively, and participants with a higher prior guess also tended to update their beliefs negatively. 

This might be due to **spillover effects**, where the information provided diffuses beyond the treatment group.
Nevertheless, the much steeper regression line for the treatment group indicates that the update in beliefs within the control group is small in comparison.

Something we need to be careful about is, whether the treatment effect on beliefs we are examining is heterogeneous.

#< info "Heterogenous treatment effects"
Randomized field experiments usually report an average treatment effect, even though the effect size might vary for individuals. Heterogeneous treatment effects are defined as nonrandom variability in the direction or magnitude of a treatment effect (Varadhan R, Seeger JD, 2013). In other words, different subgroups or subjects with different characteristics might experience stronger treatment effects than others.
#>

#< quiz "Heterogenous treatment effects"
question: Do we observe heterogeneous treatment effects in beliefs in the binned scatterplot?
sc:
- yes*
- no
- not observable in this graph
success: Great, your answer is correct!
failure: Try again.
#>

On average, subjects with a higher deviation from the 17% truth also updated their belief significantly more than those who were closer to the information provided. This means that we indeed observe heterogeneity in the treatment effect for prior beliefs. In this particular instance, heterogeneity in prior beliefs is intuitive as there is obviously a lot more room for a correction of beliefs among outliers, however we need to keep it in mind when estimating the causal effects later.

For an overview of the change of beliefs for control and treatment group dependent on whether their initial guess was above or below the true 17% between part one and two of the experiment, check the chunk below.

**Task:** Check the chunk to plot the graph.

```{r}
#< task_notest
dat<-data%>%
  mutate(belief_part_a17 = ifelse(belief_planned_part_hkust >= 17, prior_belief_part_hkust, NA),
         belief_part_b17 = ifelse(belief_planned_part_hkust < 17, prior_belief_part_hkust, NA),
         belief_part_ps_a17 = ifelse(belief_planned_part_hkust >= 17, posterior_belief_part_hkust, NA),
         belief_part_ps_b17 = ifelse(belief_planned_part_hkust < 17, posterior_belief_part_hkust, NA))%>%
  group_by(treat)%>%
  summarise(m_above__0 = mean(belief_part_a17, na.rm = TRUE),
            m_below__0 = mean(belief_part_b17, na.rm = TRUE),
            m_above__1 = mean(belief_part_ps_a17, na.rm = TRUE),
            m_below__1 = mean(belief_part_ps_b17, na.rm = TRUE),
            sd_above__0 = sd(belief_part_a17, na.rm = TRUE),
            sd_below__0 = sd(belief_part_b17, na.rm = TRUE),
            sd_above__1 = sd(belief_part_ps_a17, na.rm = TRUE),
            sd_below__1 = sd(belief_part_ps_b17, na.rm = TRUE),
            c_above__0 = sum(!is.na(belief_part_a17)),
            c_below__0 = sum(!is.na(belief_part_b17)),
            c_above__1 = sum(!is.na(belief_part_ps_a17)),
            c_below__1 = sum(!is.na(belief_part_ps_b17)))

dat_longer <- pivot_longer(dat, cols = c("m_above__0", "m_below__0", "m_above__1", "m_below__1"), 
                          names_to = c("measure", "posterior"), 
                          names_sep = "__")
dat_longer <- dat_longer%>%
  mutate(type = paste0(measure, treat),
         stat = ifelse(treat == 1, "Treatment", "Control"),
         sd = case_when(str_detect(type, "above") & posterior == 1 ~ sd_above__1,
                        str_detect(type, "above") & posterior == 0 ~ sd_above__0,
                        str_detect(type, "below") & posterior == 1 ~ sd_below__1,
                        str_detect(type, "below") & posterior == 0 ~ sd_below__0
                        ),
         count = case_when(str_detect(type, "above") & posterior == 1 ~ c_above__1,
                        str_detect(type, "above") & posterior == 0 ~ c_above__0,
                        str_detect(type, "below") & posterior == 1 ~ c_below__1,
                        str_detect(type, "below") & posterior == 0 ~ c_below__0
                        ),
         ci = 1.96*(sd/sqrt(count)))%>%
  select(measure, posterior, value, type, stat, sd, count, ci)


ggplot(data = dat_longer, aes(x = posterior, y = value, group = type))+
  geom_point(aes(shape = stat, colour = stat), size = 2.8)+
  geom_line(aes(linetype = stat, colour = stat), linewidth = 0.6)+
  geom_errorbar(aes(ymin = value - ci, ymax = value + ci), width = 0.03, color = "gray")+
  scale_shape_manual(values = c(20,18))+
  scale_linetype_manual(values = c("dashed","solid"))+
  scale_color_manual(values = c("#000000", "#990000"))+
  scale_y_continuous(limits = c(0,40))+
  scale_x_discrete(expand = c(0.07,0.07),
                   labels = c("2016/06/24", "2016/06/30"))+
  labs(x = element_blank(),
       y = "Belief re: % students who actually participate",
       title = "Figure 4: Treatment Effect on First Stage")+
  theme_classic()+
  theme(legend.title = element_blank(),
        legend.position = "bottom")+
  annotate("text", x = 1.8, y = 35, label = "Prior belief re: planned \n participation > 17 (truth)")+
  annotate("text", x = 1.8, y = 4, label = "Prior belief re: planned \n participation < 17 (truth)")
#>
```

The graph shows the prior and posterior beliefs on actual protest participation among survey participants for the treatment and control group, split into two subsamples. The subsamples are divided by whether subjects prior beliefs regarding planned participation were above (or below) the true 17%. 
We can once again observe that the updating of beliefs systematically differs in sign between the two subgroups of the treatment group.
This might become a problem, when estimating the treatment effect, for example with a linear regression. The effects in the respective groups might offset each other, thus leading to biased estimates.

Now that we have visualized the treatment effect on posterior beliefs, we want to estimate it using **linear regressions**. The two main takeaways from the descriptive part of this exercise are, that we are dealing with **offsetting treatment effects** and **heterogeneity**. 

## Inferential statistics

Now that we have observed the treatment effect in the descriptive analysis, we want to quantify the treatment effect of truthful information on posterior beliefs using **linear regressions**.

Before we start, we need to reduce the sample to subjects, whose posterior beliefs were elicited in the followup.

**Task:** Check the chunk below.

```{r}
#< task_notest
dat<-data%>%
  filter(followup_postjuly1st_w3 == 1 & !is.na(posterior_belief_part_hkust))
#>
```

To estimate the treatment effect on posterior beliefs we use a simple specification:

$$
posterior\\_belief\\_part\\_hkust\_i = ÃŸ\_1treat\_i + \varepsilon\_{i}
$$

**Task:** Fill in the blanks to estimate the regression above. 

```{r}
#< fill_in
reg1 <- lm(___ ~ ___, data = dat)
#>
reg1 <- lm(posterior_belief_part_hkust ~ treat, data = dat)
```

In this problem set we are going to be using the package `modelsummary` to create the regression tables.

#< info modelsummary
`modelsummary` provides a convenient way to generate summary tables for regression models. It allows us to compare and display multiple regression models and includes great utilities to customize the information displayed. Besides cosmetics, users are able to rename, reorder and omit parameters, calculate robust standard errors and confidence intervals, or add rows with supplemental information to the table.
The `modelsummary` also combines with packages like `kableExtra`, so you can customize the appearance of the table using its functions as well.

If you need more information on the package and its use, simply click [here](https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html).
#>

**Task:** For an example on how to use `modelsummary`, check the chunk below

```{r}
#< task_notest
library(modelsummary)
modelsummary(reg1,
             output = "kableExtra",
             coef_omit = 1,
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             gof_map = c("nobs", "r.squared"))
#>
```

In this specification we estimate a treatment effect of 0.691 percentage points on posterior beliefs. This means, that being part of the treatment group corresponds with a 0.691 percentage point higher posterior belief among students. The model measures the average treatment effect among the whole sample, ignoring **offsetting effects** and **heterogeneity**. Looking at figure 4 we expect a large positive effect for lower priors and an even larger negative effect for higher priors.

The easiest way of dealing with both **offsetting treatment effects** and **heterogeneity** causing our treatment coefficient to be hard to interpret, is to run separate regressions for the respective subsamples.

**Task:** Fill in the blanks to estimate the regression for subjects with higher and lower prior beliefs on planned participation respectively.

```{r}
#< fill_in
reg2 <- lm(posterior_belief_part_hkust ~ treat, data = filter(dat, ___ < 17))
reg3 <- lm(posterior_belief_part_hkust ~ treat, data = filter(dat, ___ >= 17))
#>
reg2 <- lm(posterior_belief_part_hkust ~ treat, data = filter(dat, belief_planned_part_hkust < 17))
reg3 <- lm(posterior_belief_part_hkust ~ treat, data = filter(dat, belief_planned_part_hkust >= 17))
```

**Task:** Check the chunk below.

```{r}
#< task_notest
modelsummary(list("Priors below truth" = reg2, "Priors below truth" = reg3),
             output = "kableExtra",
             coef_omit = 1,
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "Table 3: Treatment Effect on Posterior Beliefs",
             gof_map = c("nobs", "r.squared"))
#>
```

As expected, we estimate a positive treatment effect for lower priors and a much bigger negative effect for higher priors, with both coefficients being significant at 1%. For lower priors, treatment moved posterior beliefs by 4.409 percentage points. For higher priors, being part of the treatment group decreases posterior beliefs by 9.140 percentage points. Both subgroups updated their beliefs towards the true 17%, students evidently trust the information provided. This is important, because it allows us to estimate the causal effects of beliefs on participation later on, as we can estimate whether this experimentally induced change in beliefs changes their prospects of joining the protest on July 1st. 

An alternative way of dealing with **offsetting treatment effects** is to use a direction adjusted treatment indicator. This allows us to estimate the monotonic treatment effect for the entire sample, and not separate effects for subgroups. We create the direction adjusted treatment variable `treat_im`, setting it as -1 for subjects whose prior beliefs on planned participation were higher than the true level of 17% who were treated, 1 for lower priors from the treatment group, and 0 for the control group. We know from the descriptive part of this exercise, that priors above 17% updated their beliefs negatively. This allows us to estimate a monotonic treatment by flipping the sign for for high priors, where negative effects are expected to occur. 

**Task:** Fill in the gaps to create `treat_im`.

```{r}
#< fill_in
dat<-dat%>%
  mutate(treat_im = ifelse(belief_planned_part_hkust >= 17 & treat == 1, ___, ___))
#>

dat<-dat%>%
  mutate(treat_im = ifelse(belief_planned_part_hkust >= 17 & treat == 1, -1, treat))
```

Now that we have created the direction adjusted treatment effect, we can estimate the monotonized treatment effect for the whole sample. The treatment effect, we estimated for the whole sample without direction adjusted treatment in the first regression, amounts to 0.691 percentage points.

#< quiz "Monotonic treatment effect"
question: Is the monotonic treatment effect higher or lower than 0.691?
sc:
- higher
- lower*
success: Great, your answer is correct!
failure: Try again.
#>

**Task:** Check the chunk below to add the direction adjusted treatment to the table above.

```{r}
#< task_notest
reg1 <- lm(posterior_belief_part_hkust ~ treat_im, data = dat)

modelsummary(list("All Subjects" = reg1, "Prior below truth" = reg2, "Prior above truth" = reg3),
             output = "kableExtra",
             coef_omit = "^(?!.*treat)",
             coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "Table 3: Treatment Effect on Posterior Beliefs",
             gof_map = c("nobs", "r.squared"))%>%
add_header_above(c(" " = 1, "Posterior belief on participation among HKUST students (%)" = 3))
#>
```

Surprisingly, we estimate a monotonic treatment effect of negative 2.865 percentage points. This estimate stands in stark contrast to our findings in the regression using the normal treatment variable `treat`. If anything, the monotonic treatment effect should be larger, as we correct the **offsetting effects**.

Normally, if randomization was successful, treatment would not be correlated with the error term, and we would not need to add control variables to the equation to find a consistent estimator for the causal effect on beliefs. However, the authors of the underlying paper counter the distorted monotonic treatment effect by controlling for `prior_belief_part_hkust`, `above17` and their interaction. 

**Task:** Fill in the blanks to create the dummy `above17` and the interaction term `prior_belief_part_hkustXabove17`.

```{r}
#< fill_in
dat<-dat%>%
  mutate(above17 = ifelse(belief_planned_part_hkust >=17 & !is.na(belief_planned_part_hkust),1,0),
         prior_belief_part_hkustXabove17 = ___)

head(dat[, c("prior_belief_part_hkust", "belief_planned_part_hkust", "above17", "prior_belief_part_hkustXabove17")], n = 5)
#>
dat<-dat%>%
  mutate(above17 = ifelse(belief_planned_part_hkust >=17 & !is.na(belief_planned_part_hkust),1,0),
         prior_belief_part_hkustXabove17 = prior_belief_part_hkust * above17)

head(dat[, c("prior_belief_part_hkust", "belief_planned_part_hkust", "above17", "prior_belief_part_hkustXabove17")], n = 5)
```

But why do they do this? As we already observed in the balance check, the treatment indicator `treat` should be well randomized, and earlier estimates for the two subgroups should estimate a causal effect. However, when estimating the direction adjusted treatment effect with `treat_im`, the results are subject to **omitted variable bias** as we generated the variable based on prior beliefs. **Omitted variable bias** is a bias in the estimator that occurs when the explanatory variable is correlated with one or more omitted variables in the error term (Christoph Hanck et al.). Effects of omitted variables will be attributed to the the explanatory variables included, leading to inconsistent estimators. For a more detailed explanation of the **omitted variable bias** click [here](https://www.econometrics-with-r.org/6-1-omitted-variable-bias.html).

Let us take a look at the correlation between the direction adjusted treatment indicator and the attributes we used to generate it.

**Task:** Check the chunk below to calculate the correlation matrix for the variables of interest.

```{r}
#< task_notest
df<-data.frame("posterior_belief_part_hkust" = dat$posterior_belief_part_hkust,
               "treat" = dat$treat,
               "treat_im" = dat$treat_im,
               "prior_belief_part_hkust" = dat$prior_belief_part_hkust,
               "above17" = dat$above17,
               "prior_belief_part_hkustXabove17" = dat$prior_belief_part_hkustXabove17)
cbind(data.frame("Correlation" = colnames(df)), as.data.frame(cor(df)))
#>
```

As expected, our normal treatment coefficient seems to be well randomized, as we do not find significant correlation with either prior beliefs or `above17`. The treatment effect we estimated split for the two subgroups are not subject to biases arising from omitting confounding variables. 

However, there is a strong negative correlation between our direction adjusted treatment indicator `treat_im` and the control variables `prior_belief_part_hkust` (-0.65), `above17`(-0.8). The interaction between prior beliefs and `above17` is therefore also negatively correlated with `treat_im`. This is not surprising, as we generated the directional treatment indicator as a function of prior beliefs and whether those were above 17. Both variables are also correlated with our dependent variable of posterior beliefs. By omitting these confounders in our initial regression, we received an inconsistent estimator. In order to capture the unbiased monotonic treatment effect on beliefs, we have to control for these confounders in our regression.

**Task:** Fill in the blanks to estimate the regression controlling for the confounders.

```{r}
#< fill_in
reg4 <- lm(___ ~ ___ + ___ + ___ + prior_belief_part_hkustXabove17, data = dat)
#>
reg4 <- lm(posterior_belief_part_hkust ~ treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat)
```

Even though we do not necessarily need to add the controls when using `treat` and the sample split, let us run the regressions regardless, adding all three results to the table above in a new panel called "Baseline +". 

**Task:** Check the chunk.

```{r}
#< task_notest
reg5 <- lm(posterior_belief_part_hkust ~ treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 0))
reg6 <- lm(posterior_belief_part_hkust ~ treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 1))

modelsummary(list("Panel A: Baseline" = list("All Subjects" = reg1, "Prior below truth " = reg2, "Prior above truth " = reg3), "Panel B: Baseline +" = list("All Subjects" = reg4, "Prior below truth " = reg5, "Prior above truth " = reg6)), 
         output = "kableExtra",
         coef_omit = "^(?!.*treat)",
         coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
         gof_map = NA,
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
         shape = "rbind",
         title = "Table 3: Treatment Effect on Posterior Beliefs")%>%
  add_header_above(c(" " = 1, "Posterior belief on participation among HKUST students (%)" = 3))
#>
```

In comparison to the baseline, the direction adjusted treatment effect is no longer negative, because we included the confounding factors in the regression. With an average monotonic treatment effect of 5.891 percentage points, the effect size is between the absolute values the two subgroups coefficients, which is where we expect it to be. 
Thus far, we have focused on the treatment effect on beliefs regarding other students participation. When estimating the treatment effect on students beliefs regarding participation among the entire HK population, the results are consistent with the findings above. You can find the estimates in the Appendix (A1).

Please note, that we did not use robust standard errors in the regression above. Most empirical papers in economics use some form of robust standard errors, the same applies for the paper our analysis is based on. Robust standard errors are especially relevant if heteroskedasticity is present in the model, so the error term is not identically, independently normal distributed. Robust standard errors provide more accurate and reliable inference, allowing researchers to make sound conclusions. `vcov` allows us to specify the type of robust standard error we want to use for our model within the `modelsummary()` statement. From now on we will be using **HC1** a heteroskedasticity-consistent type of standard error.

**Task:** Fill in the blanks to add robust standard errors to the table above.

```{r}
#< fill_in
modelsummary(list("Panel A: Baseline" = list("All Subjects" = reg1, "Prior below truth " = reg2, "Prior above truth " = reg3), "Panel B: Baseline +" = list("All Subjects" = reg4, "Prior below truth " = reg5, "Prior above truth " = reg6)), 
         output = "kableExtra",
         ___,
         coef_omit = "^(?!.*treat)",
         coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
         gof_map = NA,
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
         shape = "rbind",
         title = "Table 3: Treatment Effect on Posterior Beliefs")%>%
  add_header_above(c(" " = 1, "Posterior belief on participation among HKUST students (%)" = 3))
#>
modelsummary(list("Panel A: Baseline" = list("All Subjects" = reg1, "Prior below truth " = reg2, "Prior above truth " = reg3), "Panel B: Baseline +" = list("All Subjects" = reg4, "Prior below truth " = reg5, "Prior above truth " = reg6)), 
         output = "kableExtra",
         vcov = "hc1",
         coef_omit = "^(?!.*treat)",
         coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
         gof_map = NA,
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
         shape = "rbind",
         title = "Table 3: Treatment Effect on Posterior Beliefs")%>%
  add_header_above(c(" " = 1, "Posterior belief on participation among HKUST students (%)" = 3))
```

#< quiz "Robust standard errors"
question: Compare the regression results to the table above. What has changed?
sc:
- The estimators of the regression coefficients.
- The standard errors of the estimators of the regression coefficients.*
- Nothing.
success: Great, your answer is correct!
failure: Try again.
#>

While the coefficients stayed the same, standard errors increased slightly. In general, robust standard errors do not change the coefficients. 


### Robustness of the estimated treatment effects

Next we want to examine the robustness of these first stage estimates. We do this by comparing the results with two other panels: Panel C will include a plethora of control variables for subjects background characteristics and economic status, namely `gender`, `birth_year`, `monthly_income_total_w3`, `realest_totalowned_w3`, `father_educ_hsabove_w123`, `mother_educ_hsabove_w123`, `hs_language_english`, `hkgeneration_w123`. In Panel D we remove extreme priors.

Working with so many control variables in Panel C can be tiresome and chaotic. The `glueformula` package offers a convenient solution, as we can simply save the variable names in a string vector adding them to the regression altogether.

**Task** Check the chunk to load the `glueformula` package and save the control variables in `control`.

```{r}
#< task_notest
library(glueformula)

control <- c("gender", "birth_year", "monthly_income_total_w3", "realest_totalowned_w3", "father_educ_hsabove_w123", "mother_educ_hsabove_w123", "hs_language_english", "hkgeneration_w123")
#>
```

Now we can simply create the formula for Panel C by adding the container, we created above in braces,to the initial regression in the `gf()` function and subsequently call the regression with `formula_im`.

**Task:** Fill in the blanks to estimate the extended models.

```{r}
#< fill_in
#All Subjects
formula_im <- gf(posterior_belief_part_hkust ~ treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + ___)
reg7<-lm(___, data = dat)
#Prior below/above truth
formula <- gf(posterior_belief_part_hkust ~ ___ + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + ___)

reg8<-lm(formula, data = filter(dat, belief_planned_part_hkust < 17))
reg9<-lm(formula, data = filter(dat, belief_planned_part_hkust >= 17))
#>
formula_im <- gf(posterior_belief_part_hkust ~ treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + {control})
reg7<-lm(formula_im, data = dat)
formula <- gf(posterior_belief_part_hkust ~ treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + {control})
reg8<-lm(formula, data = filter(dat, belief_planned_part_hkust < 17))
reg9<-lm(formula, data = filter(dat, belief_planned_part_hkust >= 17))

```

In Panel D we create a trimmed version of the data set, discarding outliers. The authors of the paper remove observations with the 5% highest and lowest prior beliefs on planned participation among students. Subsequently, we estimate the regression models based on the trimmed data set to filter out students who might not have taken the surveys seriously, guessing 100% planned participation among the sample for instance.

**Task:** Check the chunk to estimate a regression based on a trimmed version of the data set.

```{r}
#< task_notest
#Calculate 5th and 95th percentile
q05<-quantile(dat$belief_planned_part_hkust, 0.05)
q95<-quantile(dat$belief_planned_part_hkust, 0.95)
#Create trimmed variable without the 5% highest and lowest prior beliefs
dat<-dat%>%
  mutate(belief_planned_part_hkust_tr = ifelse(belief_planned_part_hkust < q05 | belief_planned_part_hkust > q95, NA,
                                                belief_planned_part_hkust))
#Create trimmed version of the data set 
dat_trimmed<-dat%>%
  filter(!is.na(belief_planned_part_hkust_tr))
#Estimate the coefficients
reg10<-lm(posterior_belief_part_hkust ~ treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat_trimmed)
reg11<-lm(posterior_belief_part_hkust ~ treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat_trimmed, belief_planned_part_hkust < 17))
reg12<-lm(posterior_belief_part_hkust ~ treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat_trimmed, belief_planned_part_hkust >= 17))
#>
```

Now we can add both panels to our table.

**Task:** Check the chunk to create the regression table. 

```{r}
#< task_notest
modelsummary(list("Panel A: Baseline" = list("All Subjects" = reg1, "Prior below truth " = reg2, "Prior above truth " = reg3), "Panel B: Baseline +" = list("All Subjects" = reg4, "Prior below truth " = reg5, "Prior above truth " = reg6), "Panel C: With Controls" = list("All Subjects" = reg7, "Prior below truth " = reg8, "Prior above truth " = reg9), "Panel D: Trimmed" = list("All Subjects" = reg10, "Prior below truth " = reg11, "Prior above truth " = reg12)), 
         output = "kableExtra",
         vcov = "hc1",
         coef_omit = "^(?!.*treat)",
         coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
         gof_map = NA,
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
         shape = "rbind",
         title = "Table 3: Treatment Effect on Posterior Beliefs")%>%
  add_header_above(c(" " = 1, "Posterior belief on participation among HKUST students (%)" = 3))

#>
```

Well executed randomization allows for causal interpretation of our OLS results. In the balance check in the last exercise, we already observed that control and treatment group where balanced before the intervention. The two panels we just added, allow us to perform a so called *sensitivity analysis* to evaluate the randomization of the the underlying data on the basis of our regression results. The question we are raising is, whether the effects change, if control variables are added or extreme priors are removed. In other words, how the models estimates react to fluctuations in parameters and the data it is based upon (Salciccioli et al.).

#< quiz "Randomization in our first stage estimates"
question: Does the sensitivity analysis indicate good randomization?
sc:
- yes*
- no
success: Great, your answer is correct!
failure: Try again.
#>

Adding control variables in Panel C does not meaningfully alter estimates, neither for the entire population nor for the subgroups. This means that we do not find significant variation in individual characteristics, like gender, income, or the parents educational background, thus proving that the treatment assignment was well randomized. Though coefficients in Panel D are smaller, especially for priors above 17%, removing extreme observations still did not greatly affect the results, as smaller changes are to be expected if the underlying data changes. 


### Randomization inference

An additional concern is whether the statistical inferences are sound in our randomized field experiment . We can use **randomization inference** to address this concern: It is a nonparametric approach to assess the validity of statistical hypotheses, that does not rely on assumptions about the underlying datas distribution or the statistical model (Deaton and Cartwright, 2018).

In this case we are going to be comparing the observed test statistics with the distribution of test statistics, generated by repeated randomized treatment assignment. This allows us to verify, whether the inferences from our model above are accurate.

As generating the t statistics for 10000 random treatment assignments might take a couple of minutes, we will just load a prepared data set with the results below. If you want to know how the data used in the next plot was created regardless, you can click on the info below.

#< info "Randomization first stage inference"

The first step is to create the random treatment variable `treat_r`. In the experiment two thirds of participants were assigned to the treatment group and one third to the control group. Therefore we randomly assign treatment status to two thirds of all subjects in the sample. The `runif()` function in its default setting creates random numbers between 0 and 1, we store it in `random_treatment`. This allows us to easily assign treatment status to subjects with a higher `random_treatment` value than 0.33.

Next step is to estimate the effects of our randomized treatment variable on posterior beliefs, and to extract the t value for both lower priors and higher priors adding them to a dataframe.

In order to receive robust estimates we now repeat the process as often as possible.

```{r eval=FALSE}
#Setting a seed
set.seed(1)
randomization_1ststage_inference<- 0

for (i in 1:10000) {
  
#Random Treatment assignment
dat_sub <- dat %>%
  mutate(random_treatment = runif(nrow(dat)),
         treat_r = ifelse(random_treatment > 0.33, 1, 0))
  
#Estimating the regression models for the randomized treatment variable
below_mod <- lm(formula = posterior_belief_part_hkust ~ treat_r + prior_belief_part_hkust, data = filter(dat_sub, belief_planned_part_hkust < 17))
above_mod <- lm(formula = posterior_belief_part_hkust ~ treat_r + prior_belief_part_hkust, data = filter(dat_sub, belief_planned_part_hkust >= 17))
  
#Extracting t-values adding them to a dataframe
sim_dat<-data.frame(t_below = summary(below_mod)$coefficients[2,"t value"], t_above = summary(above_mod)$coefficients[2,"t value"])
sim_dat$i <- i
randomization_1ststage_inference <- rbind(randomization_1ststage_inference, sim_dat)

}

randomization_1ststage_inference<-filter(randomization_1ststage_inference, i != 0)
```
#>

After obtaining the t values for 10000 random treatment assignments for lower and higher priors respectively, the next step is to plot their distribution to compare them to the t value of the actual treatment effect, we estimated in the regression table above.

#< quiz "First stage randomization inference"
question: If our inferences from Table 3 are sound, the t values of retroactive random assignment are...
sc:
- evenly distributed 
- distributed around the t value of the actual assignment
- distributed around 0*
success: Great, your answer is correct!
failure: Try again.
#>

**Task:** Check the chunk to plot the distribution of t statistics.

```{r}
#< task_notest
randomization_1ststage_inference <- readRDS("data/random_1ststage_inference.rds")

plt1<-randomization_1ststage_inference%>%
  ggplot(aes(x = t_below)) + 
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white", bins = 20) + 
    geom_vline(xintercept = 8.4, colour = "red")+
    theme_classic()+
    scale_x_continuous(breaks = c(-2.5, 0, 2.5, 5, 7.5))+
    xlab("t-stats")+
    ylab("Density")+
    labs(title = "Prior belief below truth")

plt2<-randomization_1ststage_inference%>%
  ggplot(aes(x = t_above)) +
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white", bins = 20) + 
    theme_classic()+
    geom_vline(xintercept = -7.3, colour = "red")+
    xlab("t-stats")+
    ylab("Density")+
    labs(title = "Prior belief above truth")
library(gridExtra)
grid.arrange(plt1,plt2,ncol = 2, bottom = "Figure 5: Randomization first stage inference")
#> 
```

<br/>

In this barplot we can see the distribution of t statistics for our randomized treatment effect split between subjects with prior beliefs below the truth and above the truth. The red line marks the t value belonging to the actual treatment assignment.
Both the t statistics for lower and higher priors are distributed around zero, far off from the treatment effects t value estimated via linear regression. Therefore, we do not find similarly significant effects when randomly reallocating treatment assignments, which validates the statistical inferences from our model. 

After estimating the treatment effect on posterior beliefs in this exercise, observing a change in beliefs towards the true information provided, we shift our attention towards the treatment effect on participation in the next exercise.



## Exercise 5. How does truthful information affect actual turnout?

<br/>

In the first stage of the experiment, we examined the effects of truthful information on posterior beliefs regarding others participation. We observed a positive effect for subjects with prior beliefs below the true 17% planned participation, while the treatment reduced beliefs for higher priors. Now we want to take look at the treatment effect on a subjects own participation. 

Before we begin we need to load the data set again.

**Task:** Check the chunk to load the data set.

```{r}
#< task_notest
data<-readRDS("data/data2.RDS")
#>
```

First of all, let us check how many of the students from our sample actually participated.

**Task:** Calculate the turnout among students.

```{r}
#< task
#Remove observations where participation was elicited
dat <- filter(data, !is.na(part))
#Calculate the participation among the sample

#>
mean(dat$part)
```

Roughly 3% of students in the experiment reported that they actually participated. Considering the fact that approximately 17% of subjects stated before the intervention that they were planning to participate, this is quite a large discrepancy. The question is, whether only a small fraction of subjects planning to participate actually following through is experimentally induced. Some level of attrition might be normal in the context of antiauthoritarian protests and a rather high risk of crackdown.

To answer this question, we take a more detailed look at turnout levels by dividing the sample into 4 different subgroups, treatment and control for lower and higher priors respectively.

**Task:** Complete the code to compute the mean, standard deviation and number of observations for each combination of `above17` and `treat` and show the results.

```{r}
#< fill_in
dat <- data%>%
  group_by(___, ___)%>%
  summarise(mean = ___,
            sd = sd(part, na.rm = TRUE),
            count = n())

___
#>
dat <- data%>%
  group_by(above17, treat)%>%
  summarise(mean = mean(part, na.rm = TRUE),
            sd = sd(part, na.rm = TRUE),
            count = n())

dat
```

At first glance, attendance is highest for treated subjects with high priors, but lowest for treated subjects with lower priors.
We can visualize the average turnout in each of the respective groups, by creating a barplot using `geom_bar()`,  adding 90% confidence intervals to each bar.

**Task:** Check the chunk below.

```{r}
#< task_notest
dat$id <- 1:4

dat%>%
  ggplot(data = dat, mapping = aes(x = as.character(id), y = mean, fill = as.factor(above17), group = above17))+
  geom_bar(stat = "identity")+
  geom_errorbar(aes(ymin = mean + (qt(0.95, df = count - 1)*(sd/sqrt(count))),
                    ymax = mean - (qt(0.95, df = count - 1)*(sd/sqrt(count)))),
                width = 0.06)+
  annotate(geom = "text", x = "1", y = 6, label = "")+
  annotate(geom = "text", x = "3", y = 8.5, label = "")+
  labs(x = "", y = "% participated in July 1st March 2016", title = "Figure 6: Treatment effect on protest participation")+
  scale_y_continuous(breaks = c(0,2,4,6,8,10), limits = c(-0.573,10))+
  scale_x_discrete(labels = c("1" = "Control", "2" = "Treatment", "3" = "Control", "4" = "Treatment"))+
  scale_fill_discrete(name = element_blank(), labels = c("Prior belief below truth", "Prior belief above truth"))+
  theme_classic()+
  theme(legend.position = "bottom")
#>
```

This barplot shows the actual self-reported participation in the July 1st protest of 2016, split on the basis of prior beliefs and treatment status including the 90% confidence interval. One can see a significantly worse turnout among subjects from the treatment group with lower priors in comparison with subjects from the control group. Meanwhile, in the subsample with prior beliefs above the true 17% we can observe a large increase in turnout.

#< quiz "Effect on beliefs vs participation"
question: Do the treatment effects on beliefs and participation move in the same direction?
sc:
- yes
- no*
success: Great, your answer is correct!
failure: Try again.
#>

The sign of the treatment effect differs between beliefs and participation for both subgroups. While lower priors updated their beliefs positively, they seem to participate less. Higher priors, who updated their beliefs negatively seemed to have turned out in large numbers. 

#< quiz "Effects on turnout"
question: This indicates a game of... 
sc:
- strategic complements
- strategic substitutes*
success: Great, your answer is correct!
failure: Try again.
#>

To reiterate, in a game of **strategic complements**, a more aggressive strategy by one player increases the marginal profit for other players. In this particular experimental setting, others planned attendance being higher than expected would persuade treated individuals to take part as well. On a contrary, in a game of **strategic substitutes**, a more aggressive strategy decreases the marginal profit of other players (Bulow et. al., 1985). 
In this setting, we can observe that subjects from the treatment group, who underestimated the planned participation, had a lower average attendance than low priors from the control group, which strongly suggests **strategic substitutability**. More people planning to attend than expected, seems to have had a negative effect on their own commitment. For subjects, who overestimated planned participation, we observe a large positive treatment effect, despite receiving information that the protest would probably be smaller than expected.

This suggests that the protest decision â€œis a negative function of beliefs regarding the turnout of othersâ€ (Cantoni et. al.).

We can test this hypothesis on the relationship between treatment and participation by estimating the reduced form linear regression. The reduced form in this context refers to the effect of treatment on actual participation in our baseline specification:

$$
part = ÃŸ_1treat + \varepsilon
$$
We proceed similar to the regressions from last exercise, estimating the baseline regression for all subjects, priors above and below the truth, then adding controls and using a trimmed data set in separate panels. Lets start by reducing the sample to subjects where the followup and posterior beliefs were elicited.

**Task** Check the chunk below.

```{r}
#< task_notest
dat <- data%>%
  filter(followup_postjuly1st_w3 == 1 & !is.na(posterior_belief_part_hkust))
#>
```

Now we can estimate the baseline regression as described above. The barplot in figure 6 indicates, that we are dealing with **offsetting treatment effects** again, as treatment increased participation for higher priors, yet decreased participation for lower priors. We proceed analogous to the last exercise in using both a treatment adjusted treatment indicator and a split sample.

**Task:** Fill in the blanks to estimate the the treatment effect on participation for all subjects, lower and higher priors.

```{r}
#< fill_in
#Whole sample
reg1<-lm(part ~ ___, data = dat)
#Prior below truth
reg2<-lm(part ~ ___, data = filter(dat, above17 == 0))
#Prior above truth
reg3<-lm(part ~ ___, data = filter(dat, above17 == 1))
#>
reg1<-lm(part ~ treat_im, data = dat)
reg2<-lm(part ~ treat, data = filter(dat, above17 == 0))
reg3<-lm(part ~ treat, data = filter(dat, above17 == 1))
```

**Task:** Fill in the blanks to create the regression table.

```{r}
#< fill_in
modelsummary(list("All Subjects" = ___,"Prior below truth" = ___,"Prior above truth" = ___),
      output = "kableExtra",
      ___ = "hc1",
      coef_omit = "^(?!.*treat)",
      coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
      gof_map = c("nobs", "r.squared"),
      stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
      title = "Table 4: Treatment effect on protest participation")%>%
  add_header_above(c(" " = 1, "Participated in 2016 July 1 March" = 3))
#>
modelsummary(list("All Subjects" = reg1,"Prior below truth" = reg2,"Prior above truth" = reg3),
      output = "kableExtra",
      vcov = "hc1",
      coef_omit = "^(?!.*treat)",
      coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
      gof_map = c("nobs", "r.squared"),
      stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
      title = "Table 4: Treatment effect on protest participation")%>%
  add_header_above(c(" " = 1, "Participated in 2016 July 1 March" = 3))
```

#< quiz "Monotonic treatment on participation"
question: Direction adjusted treatment...
sc:
- caused participation to decrease by 2.713 percent
- caused participation to decrease by 2.713 percentage points*
success: Great, your answer is correct!
failure: Try again.
#>

We estimate the monotonic treatment effect to be -2.713 percentage points. The coefficient is significant at 1%. In the last exercise we estimated a monotonic belief change north of 5 percentage points due to informational treatment. As we have already seen in the barplot created earlier, treatment significantly decreased participation among subjects whose prior beliefs were below the true 17% planned participation. For higher priors, participation increased by roughly 6.3 percentage points. Therefore, on average, students who underestimated planned participation updated their beliefs positively, but failed to show up in large numbers. Meanwhile among students who overestimated planned participation and who negatively updated their beliefs, attendance was much higher.   

That means, that treatment effects in the first stage and reduced form run in different directions, confirming the findings from the barplot. The treatment effect on participation estimated in the regression also points towards **strategic substitutability**.

### Robustness of the reduced form estimates

After estimating the treatment effect on participation, we once again check ,whether the effects vary in different specifications to identify possible confounders in personal characteristics.  

**Task:** Check the chunk to add Panel B and C to the regression table.

```{r}
#< task_notest
#Panel B
control <- c("belief_planned_part_hkust","gender", "birth_year", "monthly_income_total_w3", "realest_totalowned_w3", "father_educ_hsabove_w123", "mother_educ_hsabove_w123", "hs_language_english", "hkgeneration_w123")

formula_im <- gf(part ~ treat_im + {control})
formula <- gf(part ~ treat + {control})

reg4<-lm(formula_im, data = dat)
reg5<-lm(formula, data = filter(dat, above17 == 0))
reg6<-lm(formula, data = filter(dat, above17 == 1))
#Panel C
reg7<-lm(part ~ treat_im, data = filter(dat, !is.na(belief_planned_part_hkust_tr)))
reg8<-lm(part ~ treat, data = filter(dat, belief_planned_part_hkust < 17 & !is.na(belief_planned_part_hkust_tr)))
reg9<-lm(part ~ treat, data = filter(dat, belief_planned_part_hkust >= 17, !is.na(belief_planned_part_hkust_tr)))

modelsummary(list("Panel A: Baseline" = list("All Subjects" = reg1,"Prior below truth" = reg2,"Prior above truth" = reg3), "Panel B: With controls" = list("All Subjects" = reg4,"Prior below truth" = reg5,"Prior above truth" = reg6), "Panel C: Trimmed" = list("All Subjects" = reg7,"Prior below truth" = reg8,"Prior above truth" = reg9)),
      output = "kableExtra",
      vcov = "hc1",
      coef_omit = "^(?!.*treat)",
      coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
      gof_map = NA,
      stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
      shape = "rbind",
      title = "Table 4: Treatment effect on protest participation")%>%
  add_header_above(c(" " = 1, "Participated in 2016 July 1 March" = 3))
#>
```

None of the added control variables has had a significant impact on the treatment coefficient, which provides reassurance that the sample is indeed well randomized. In fact the monotonic treatment effect is even less significant after controlling for characteristics and beliefs on planned participation. Removing outliers in Panel C also did not significantly alter the estimated treatment effect, extreme outliers also do not have a significant impact on our estimates.

### Randomization inference

Similar to the last exercise, we want to check whether our statistical inferences from the table above are sound once again. Using randomization inference, we reassign treatment status randomly, estimating the reassigned treatment effect and its t-Statistics for lower and higher priors respectively and repeat the process 10000 times. 

#< info "Randomized reduced form inference"
```{r eval = FALSE}
set.seed(1)
randomization_reducedform_inference<- 0

for (i in 1:10000) {
  
  #Random Treatment assignment
  dat_sub <- dat %>%
    mutate(random_treatment = runif(nrow(dat)),
           treat_r = ifelse(random_treatment > 0.33, 1, 0))
  
  #Estimating the regression models for the randomized treatment variable
  below_mod <- lm(formula = part ~ treat_r, data = filter(dat_sub, above17 == 0))
  above_mod <- lm(formula = part ~ treat_r, data = filter(dat_sub, above17 == 1))
  
  #Extracting t-values adding them to a dataframe
  sim_dat<-data.frame(t_below = summary(below_mod)$coefficients[2,"t value"], t_above = summary(above_mod)$coefficients[2,"t value"])
  sim_dat$i <- i
  randomization_reducedform_inference <- rbind(randomization_reducedform_inference, sim_dat)

 }

randomization_reducedform_inference<-filter(randomization_reducedform_inference, i != 0)
```
#>

We plot the density of the t-statistics again and compare the distribution with the t-statistic of our actual treatment assignment, which is -1.78 for subjects with prior beliefs lower than the true 17% and 3.4 for subjects with higher prior beliefs.

**Task:** Check the chunk to plot the results of the randomized inference.

```{r}
#< task_notest
randomization_reducedform_inference<-readRDS("data/random_reducedform_inference.rds")

plt1<-randomization_reducedform_inference%>%
  ggplot(aes(x = t_below)) + 
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white", bins = 20) + 
    geom_vline(xintercept = -1.78, colour = "red")+
    theme_classic()+
    xlab("t-stats")+
    ylab("Density")+
    labs(title = "Prior belief below truth")

plt2<-randomization_reducedform_inference%>%
  ggplot(aes(x = t_above)) +
    geom_histogram(aes(y = ..density..), colour = "black", fill = "white", bins = 20) + 
    theme_classic()+
    geom_vline(xintercept = 3.4, colour = "red")+
    xlab("t-stats")+
    ylab("Density")+
    labs(title = "Prior belief above truth")

grid.arrange(plt1,plt2,ncol = 2, bottom = "Figure 7: Randomized reduced form inference")
#> 
```

<br/>

Once again, the t-stats are distributed around zero, which means that its unlikely that we find similarly significant effects through repeated random treatment assignment and our inferences are sound.

### Persuasion of treatment among students

Now that we have estimated it, we can benchmark the magnitude of the estimated treatment effect against estimated political mobilization effects by calculating the **persuasion rate** (DellaVigna and Gentzkow, 2010). The **persuasion rate** estimates the "percentage of receivers that change the behaviour among those who receive a message and are not already persuaded" (DellaVigna and Gentzkow, 2010). According to DellaVigna and Gentzkow, the **persuasion rate** is defined as follows:

$$
f = 100 * \frac{y_T-y_C}{e_T-e_C} *\frac{1}{1-y_0}
$$
In this experimental setting, the **persuasion rate** captures the effect of treatment on participation ( $y_T-y_C$ ), adjusting for exposure to the truthful information ( $e_T-e_C$ ) and for the size of the remaining population left to be convinced( $1-y_0$ ).

$e_T$ and $e_C$ are defined as the share of the treatment or control group, that received treatment. Obviously, in our experiment hundred percent of the treatment group received a message, while none of the subjects from the control group received information, which means $e_T$ equals one and $e_C$ equals zero.

**Task:** Check the chunk.
```{r}
#< task_notest
e_t <- 1
e_c <- 0
#>
```

For now, we want to calculate the **persuasion rate** specifically for subjects with prior beliefs higher that the truth to compare it to the treatment effect we estimated above (6.37 percentage points).
$y_T$ is defined as the mean of actual participation (`part`) among subjects from the treatment group whose prior guess on planned participation was below 17. $y_C$ is defined as the mean of participation among subjects with high priors who did not receive treatment.

**Note:** We need the average participation in percent for both groups as decimals. The participation dummy `part` is currently defined as 100 for subjects who took part and 0 for subjects who did not

**Task** Calculate $y_T$ and $y_C$.

```{r}
#< fill_in
dat_above_treat <- filter(dat, treat == 1 & above17 == 1)
y_T <- ___
dat_above_control <- filter(dat, treat == 0 & above17 == 1)
y_C <- ___
#>
dat_above_treat <- filter(dat, treat == 1 & above17 == 1)
y_T <- mean(dat_above_treat$part)/100
dat_above_control <- filter(dat, treat == 0 & above17 == 1)
y_C <- mean(dat_above_control$part)/100
```

We approximate the population left to be convinced $1-y_0$ by $1 - y_C$ as `y_C` is the share that participates without treatment.

**Task:** Fill in the blanks to calculate the **persuasion rate** `f`.

```{r}
#< fill_in
y_0 <- y_C

f <- 100* ___ * ___

f
#>
y_0 <- y_C

f <- 100* ((y_T-y_C)/(e_t-e_c)) * (1/(1-y_0))

f
```

#< quiz "Persuasion Rate"
question: Select the correct statement.
sc:
- 6% of individuals with high priors from the control group would be persuaded if treated.
- 6% of individuals with high priors would participate when treated, but would not in the absence of treatment.*
- 6% of individuals who were not already persuaded change their behaviour if treated.
success: Great, your answer is correct!
failure: Try again.
#>

We find a **persuasion rate** of 6.43% This means that more than six percent of individuals would participate when treated, but would not protest in the absence of treatment. DellaVigna and Grentzkow calculated **persuation rates** for seven different interventions. To put our findings into perspective comparing the **persuation rates**, treatment in this experiment has effects that exceed most of the different interventions calculated, such as TV content and mailings encouraging voter turnout(DellaVigna and Grentzkow, 2010).

### Heterogeneity in treatment

When looking at the treatment effect on beliefs in the last exercise, we observed that the effect was heterogeneous as it was stronger for subjects whose guess was further from the true level of 17% in both directions. We can also asses whether the treatment effect on turnout is heterogenous by disaggregating the prior beliefs into bins as we did with the binned scatterplot. This time we are going to be using the `cut()` function to slice `belief_planned_part_hkust` into 5 percentage point wide bins. The 30 percent planned participation bin should include extreme values up to 100%. We can use the `breaks` argument to specify the bins we want to create.

**Task:** Fill in the blanks to create the variable `prior_bin`.

```{r}
#< fill_in
dat_bin <- data %>%
  ___(prior_bin = ___(___, breaks = c(0, 5, 10, 15, 20, 25, 30, 100), right = FALSE))%>%
  filter(!is.na(prior_bin))
#>
dat_bin <- data %>%
  mutate(prior_bin = cut(belief_planned_part_hkust, breaks = c(0, 5, 10, 15, 20, 25, 30, 100), right = FALSE))%>%
  filter(!is.na(prior_bin))
```

Now we can aggregate the participation within each bin for the control and the treatment group respectively. First step is to create interaction terms for treatment and participation. The variable `part_treat` is only supposed to show the participation of subjects from the treatment group, every other observation should NA. `part_con` is only supposed to contain values for subjects from the control group.

**Task:** Fill in the blanks to create the interaction terms.

```{r}
#< fill_in
dat_agg <- dat_bin%>%
  mutate(part_treat = ___(treat == 1, part, NA),
         part_con = ___(treat == 0, part, NA))
#>
dat_agg <- dat_bin%>%
  mutate(part_treat = ifelse(treat == 1, part, NA),
         part_con = ifelse(treat == 0, part, NA))

```

Next step is to calculate the means for the interaction terms. The interaction terms allow us to calculate the means in the treatment and control group for each bin separately.

**Task:** Fill in the blanks to calculate the mean, sd, and count of participation in the treatment and control group and display the result.

```{r}
#< fill_in
dat_com <- dat_agg %>%
  group_by(___) %>%
  summarise(mean0 = mean(___, na.rm = TRUE),
            mean1 = mean(___, na.rm = TRUE),
            sd0 = sd(part_con, na.rm = TRUE),
            sd1 = sd(part_treat, na.rm = TRUE),
            count0 = sum(!is.na(part_con)),
            count1 = sum(!is.na(part_treat)))
dat_com
#>
dat_com <- dat_agg %>%
  group_by(prior_bin) %>%
  summarise(mean0 = mean(part_con, na.rm = TRUE),
            mean1 = mean(part_treat, na.rm = TRUE),
            sd0 = sd(part_con, na.rm = TRUE),
            sd1 = sd(part_treat, na.rm = TRUE),
            count0 = sum(!is.na(part_con)),
            count1 = sum(!is.na(part_treat)))
dat_com
```

The goal is to visualize the treatment effect in each bin. Therefore, we calculate the difference in the average participation between treatment and control group for each bin and the standard error.

**Task:** Fill in the blanks to create the variable `diff`, the difference between average participation in treatment and control group, and calculate the standard error.

```{r}
#< fill_in
dat_agg <- dat_com%>%
  mutate(diff = ___,
         se = sqrt(sd0*sd0/count0 + sd1*sd1/count1))
#> 
dat_agg <- dat_com%>%
  mutate(diff = mean1 - mean0,
         se = sqrt(sd0*sd0/count0 + sd1*sd1/count1))
```

Now that we have got the data ready, we can plot `diff` as our treatment effect for each bin, connecting the dots, adding the 95% confidence interval to each observation.

**Task:** Check the chunk to plot the results.

```{r}
#< task_notest
dat_agg$bin_name <- seq(0, 30, 5)

ggplot(data = dat_agg, aes(x = bin_name, y = diff))+
  geom_point()+
  geom_line()+
  geom_errorbar(aes(ymax = diff + 1.96*se, ymin = diff - 1.96*se), width = 0.4)+
  geom_hline(yintercept = 0, colour = "red")+
  geom_vline(xintercept = 17, colour = "red")+
  scale_y_continuous(limits = c(-10,17.5), breaks = c(-10,-5,0,5,10,15))+
  scale_x_continuous(breaks = c(0,5,10,15,20,25,30))+
  labs(x = "Prior belief re: planned participation",
       y = "Treatment effect on % participated",
       title = "Figure 8: Treatment Effect on Participation by Prior Beliefs")+
  theme_classic()
#>
```

**Note:** In the original paper the authors wrote that 95% confidence intervals were used for this figure. However, looking into the stata replication files reveals, that 90% confidence intervals were used instead. We still use 95% confidence intervals as specified in the text, so this graph does not exactly match the original.

#< quiz "Heterogeneity in treatment"
question: Do we observe heterogeneity in the treatment effect on participation? 
sc:
- Yes*
- No
success: Great, your answer is correct!
failure: Try again.
#>

As we have already seen in the barplot in the beginning of this exercise, the treatment effect is mostly negative for subjects with prior beliefs below the true 17% marked by the vertical line, and positive for subjects with higher prior beliefs. Moreover, in this graph, we observe that the effect is bigger among individuals whose prior guess was further from the true information provided, especially for subjects with high priors. This means that, consistent with our findings on the treatment effect on beliefs, the treatment effect on participation is also heterogeneous.

Another way of finding and estimating heterogeneous effects is to control for the interaction term in a linear regression. If we want to control for heterogeneous treatment effects associated with prior beliefs, we control for the interaction between the two, `treat` and `belief_planned_part_hkust`.

**Task:** Fill in the blanks to create the interaction term `treatXbelief_planned`.

```{r}
#< fill_in
dat <- dat%>%
  mutate(treatXbelief_planned = ___)
#>
dat <- dat%>%
  mutate(treatXbelief_planned = treat * belief_planned_part_hkust)
```

The next step is, to estimate the linear regression of our reduced form, controlling for prior beliefs on planned participation (`belief_planned_part_hkust`) and the interaction term.

**Task:** Estimate the regression model and save it in `reg1`.

```{r}
#< fill_in
reg1<-___

modelsummary(___,
             vcov = "hc1",
             coef_omit = "(Intercept)",
             coef_rename = c("treat" = "Treatment", "belief_planned_part_hkust" = "Prior belief re: planned participation", "treatXbelief_planned" = "Treatment x Prior beliefs"),
             gof_map = c("nobs", "r.squared"),
             title = "Table 5: Heterogeneous treatment effects by prior beliefs",
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01))
#>
reg1<-lm(part ~ treat + belief_planned_part_hkust + treatXbelief_planned, data = dat)
modelsummary(reg1,
             vcov = "hc1",
             coef_omit = "(Intercept)",
             coef_rename = c("treat" = "Treatment", "belief_planned_part_hkust" = "Prior belief re: planned participation", "treatXbelief_planned" = "Treatment x Prior beliefs"),
             gof_map = c("nobs", "r.squared"),
             title = "Table 5: Heterogeneous treatment effects by prior beliefs",
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01))
```

As you can see all three coefficients, including the interaction term, are significant at one per cent. But how do we interpret these results? Select the correct option below to finish the sentence:

#< quiz "Interaction terms"
question: A one percentage point increase in prior beliefs... 
sc:
- increases the treatment effect by 0.248 percentage points*
- increases participation by 0.248 percentage points
- decreases participation by 0.079 percentage points
success: Great, your answer is correct!
failure: Try again.
#>

After adding the interaction between treatment and prior beliefs, the treatment coefficient displays the effect for individuals with a prior belief of 0 per cent planned participation. Thus, for subjects with prior beliefs of 0 per cent, the treatment effect on participation is estimated to be -4.112 percentage points. 
The second coefficient (prior beliefs) shows the effect of prior beliefs regarding planned participation on actual participation for the control group only. A one percentage point increase in prior beliefs, significantly decreases participation by 0.079 among the control group.
The coefficient of our interaction term measures the change in the effect of treatment for a one unit change of prior beliefs. Therefore, a one percentage point increase in prior beliefs moves the treatment effect by 0.284 percentage points. In other words, the treatment effect has a larger impact on higher priors.

**Note** You might have noticed, that we can visualize the treatment effect size depending on prior beliefs with the regression results as well. As the treatment coefficient shows the treatment effect for subjects with a prior belief of zero per cent, we can use it as the intercept on the y-axis. The estimated interaction between the two variables can be used as the slope.   

**Task:** Check the chunk to add the regression results to the plot from earlier.

```{r}
#< task_notest
ggplot(data = dat_agg, aes(x = bin_name, y = diff))+
  geom_point()+
  geom_line()+
  geom_hline(yintercept = 0, colour = "red")+
  geom_abline(aes(slope = 0.248, intercept = -4.112))+
  scale_y_continuous(limits = c(-10,17.5), breaks = c(-10,-5,0,5,10,15))+
  scale_x_continuous(breaks = c(0,5,10,15,20,25,30))+
  labs(x = "Prior belief re: planned participation",
       y = "Treatment effect on % participated",
       title = "Figure 8: Treatment Effect on Participation by Prior Beliefs")+
  theme_classic()
#>
```

As you can see, the results from the regression roughly fit a line through the bins we created in figure 8. 

So far, we have attributed the heterogeneity in both the treatment effect on posterior beliefs and on actual participation to variation in prior beliefs. However, variation of individual characteristics among experimental subjects might play a role as well. For instance, people who are more altruistic might attend antiauthoritarian protests more often than less altruistic people.

The authors of the underlying paper investigate this by predicting the prior beliefs for every subject with a linear regression model, using 47 factors like for instance the aforementioned altruism, sociability and IQ. The idea behind it is to check the extent to which the treatment effects heterogeneity can be attributed to observable characteristics, rather than beliefs. The predicted priors represent this variation in individual characteristics. The residual priors represent the unexplained variation, which we suspect is driven by beliefs.

**Task:** Check the chunk below to estimate the model and save it in `fit`.

```{r}
#< task_notest
fit <- lm(belief_planned_part_hkust ~ az_democratic_support + az_independence + az_ntlidentity + az_statusquo + az_polrecent + az_polapproach + participate_umbrella_w3 + vote_legco_2016_party_democ_w3 + planned_part + az_game_identity + demosis_donation_above0_w3 + az_preference_risk + az_preference_time + az_preference_altruism + az_preference_reciprocity + az_preference_redist + az_big5_openness + az_big5_agreebleness + az_big5_conscientious + az_big5_neuroticism + az_big5_extraversion + az_cognitive_iq + az_cognitive_gpa + az_demogra_hhecon + az_demogra_ownecon + az_demogra_childenv + az_belief_institution + az_belief_protesteff + az_srec_democratic_hkust + az_sres_independence_hkust + az_sres_ntlidentity_hkust + az_sres_statusquo_hkust + az_sres_approach_hkust + az_social_polnetwork + az_social_sociability + az_srec_democratic_fri + az_sres_independence_fri + az_sres_ntlidentity_fri + az_sres_statusquo_fri + az_sres_approach_fri + az_media_frequency + az_media_source + az_pol_interest + az_pol_knowledge + gender + birth_year + religion_none, data = dat, na.action = na.exclude)
#>
```

Next, use the model saved in `fit` to predict the prior beliefs and save the residual prior beliefs for every subject.

**Task:** Fill in the blanks to create `predicted_belief_planned_part` and `residual_belief_planned_part`.

```{r}
#< fill_in
dat<-dat%>%
  mutate(predicted_belief_planned_part = ___,
         residual_belief_planned_part = ___)
#>
dat<-dat%>%
  mutate(predicted_belief_planned_part = predict(fit),
         residual_belief_planned_part = resid(fit))
```

The resulting predicted prior beliefs basically approximate what the prior belief on planned participation would have been, if it was only dependent on individual characteristics. By controlling for the interaction again, this time with predicted priors, we can see how this hypothetical prior belief interacts with treatment and if heterogeneity in treatment is to be attributed to variation in characteristics. 

Therefore, we need interaction terms for treatment with predicted and residual priors.

**Task:** Fill in the blanks to create the interaction terms.

```{r}
#< fill_in
dat<-dat%>%
  mutate(treatXpredicted = ___,
         treatXresidual = ___)
#>
dat<-dat%>%
  mutate(treatXpredicted = treat * predicted_belief_planned_part,
         treatXresidual = treat * residual_belief_planned_part)
  
```

Now that we have got the interaction terms, we can add them to the regression, along with the interacting variables separately, adding the results to our first regression above. 

**Task:** Check the chunk below.

```{r}
#< task_notest
reg2<-lm(part ~ treat + predicted_belief_planned_part + treatXpredicted, data = dat)
reg3<-lm(part ~ treat + predicted_belief_planned_part + treatXpredicted + residual_belief_planned_part + treatXresidual, data = dat)

modelsummary(list(reg1, reg2, reg3),
             output = "kableExtra",
             vcov = "hc1",
             coef_omit = c(1,3,5,7),
             coef_rename = c("treat" = "Treatment", "treatXbelief_planned" = "Treatment x Prior beliefs", "treatXpredicted" = "Treatment x predicted prior beliefs", "treatXresidual" = "Treatment x residual prior beliefs"),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "Table 5: Heterogeneous Treatment Effects by Prior Beliefs")
#>
```

In the second column we control for the interaction between treatment and predicted priors. In the third column, we add the interaction of treatment and residual priors. As we can see, the estimates for the second and third column are not as precise as in the first model. The treatment coefficients are significant at 5% instead of 1%, and the standard errors are comparatively higher. This is to be expected, as we are controlling for predicted instead of the actual priors. That being said, the treatment coefficients have the same sign in all three models.

#< quiz "Variation in beliefs"
question: Can heterogeneity in the treatment effect on participation only be attributed to individual characteristics?
sc:
- yes
- no*
success: Great, your answer is correct!
failure: Try again.
#>

Even though we find that heterogeneity is indeed driven by individual characteristics, we also observe a positive and highly significant interaction between residual priors and treatment of 0.246. This means, that there also exists heterogeneity in treatment independent from characteristics, that can be attributed to a variation in beliefs as we assumed above. 
We also find reassuring results, when repeating the process for the first stage estimates from the last exercise. The results can be found in the appendix (A2).

In the last exercise we combine first stage and reduced form to estimate the effect of posterior beliefs on actual turnout.


## Exercise 6. How do beliefs affect turnout?

<br/>

In the last two exercises we showed that providing truthful information regarding planned participation caused posterior beliefs on turnout and subjects actual participation to change significantly. We also assessed, that both effects move in opposite directions. 

The main objective of this exercise is to isolate the causal relationship between posterior beliefs and participation.

First and foremost, we need to load the data set again, reducing it to the experimental sample, i.e. subjects who received a followup and posterior beliefs were elicited.

**Task:** Check the chunk to prepare the experimental data.

```{r}
#< task_notest
dat<-readRDS("data/data2.RDS")%>%
  filter(followup_postjuly1st_w3 == 1 & !is.na(posterior_belief_part_hkust))
#>
```

In this final part of the analysis we want to estimate the causal effect of posterior beliefs, the belief of actual participation one day before the protest, on ones own participation. This leaves us with a simple regression:

$$
part_i =  ÃŸ_0 + ÃŸ_1posterior\\_belief\\_part\\_hkust_i + \varepsilon\_{i}
$$

One might expect posterior beliefs to be dependent on prior beliefs in this regression model. In the first stage, we observed that prior and posterior beliefs are positively correlated among subjects. Omitting the variable causes endogeneity problems, as posterior beliefs would be correlated with the error term, if prior beliefs were left out of the regression model. To prevent biases the authors of the paper control for the same set of variables as in the first stage, namely `prior_beliefs_part_hkust`, `above17` and the interaction term `prior_belief_part_hkustXabove17`.

**Task:** Check the chunk to run the regression.

```{r}
#< task_notest
reg1<-lm(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat)
modelsummary(reg1,
             gof_map = c("nobs", "r.squared"))
#>
```

#< quiz "Causal effect on participation"
question: Is beta1 an unbiased estimator for the causal effect of posterior beliefs on participation?
sc:
- yes
- no*
success: Great, your answer is correct!
failure: Try again.
#>

Obviously, that is a bit of a loaded question, as we cannot estimate the unbiased causal effect of posterior beliefs on participation in this specification. The reason being, that naturally occurring variation in beliefs is very likely to be endogenous (Cantoni et al., 2019) and thus correlated with the error term, leading to an inconsistent estimator $ÃŸ_1$. In order to estimate the causal effect, posterior beliefs would have to be exogenous. While we do have a lot of variables regarding individual characteristics at our disposal, it is probably impossible to control for all relevant confounders. 

However, the treatment in the experiment gives us a clear source of exogenous variation in our explanatory variable posterior beliefs. We can use this experimentally induced variation arising from treatment to our advantage, to solve our endogeneity problems using instrumental variable estimation via two stage least squares. 


### The two stage least squares method

Instrumental variable estimation via two stage least squares allows us to consistently estimate the causal effect of an endogenous explanatory variable by exploiting experimentally induced exogenous variation. Before we use the method to estimate the effects of posterior beliefs on actual participation, let us take a look at it in a general setting.

Consider a simple linear regression:
$$y_i = ÃŸ_1x_i + \varepsilon_i$$
The explanatory variable $x$ is endogenous due to unobserved confounders and therefore correlated with the error term $\varepsilon$. $ÃŸ_1$ is inconsistent.

A valid instrumental variable $z$ for the endogenous variable $x$ has to fulfill three conditions, **relevance**, **exogeneity** and the **exclusion restriction**: 

- **Relevance:** $z$ is correlated with with the endogenous variable of interest --> $cor(z_i,x_i) =Ì¸ 0$

- **Exogeneity:** $z$ is not correlated with the error term $\varepsilon$ --> $cor(z_i,\varepsilon_i) = 0$

- **Exclusion condition:** The instrument $z$ does not affect the dependent variable $y$ directly but exclusively through the endogenous variabe $x$

If the instrument satisfies all three conditions, we can use it to perform the instrumental variable regression using two stage least squares (Rieber, 2021). As the name already suggests we commence in two steps.

In the first step we regress the endogenous explanatory variable on the instrument:

$$
x = ÃŸ_0 + ÃŸ_1z + u
$$  
This regression removes the influences from $x$ that are correlated with the error term. Subsequently, we predict the values for $x$ in this regression. $\hat{x}$, the predicted values, only contain the exogenous variation induced by the instrument.

In the second step, we replace the endogenous variable with the constructed variable $\hat{x}$ we just created:

$$
y_i = ÃŸ_0 + ÃŸ_1\hat{x} + \varepsilon
$$
If all three of the conditions regarding the instrument are satisfied, $ÃŸ_1$ consistently estimates the causal effect (Kranz, 2022).

Now that we have gotten a quick overview over the method, lets get back to the topic at hand. As discussed above, we are going to be using the experimentaly induced variation to estimate the causal effect of the posterior beliefs on participation. We therefore use the treatment indicators (`treat_im` and `treat`) as instruments. Let us first check, whether the direction adjusted treatment indicator (`treat_im`) is a valid instrument. First up is **relevancy**.

**Task:** Compute the correlation between the instrument and the endogenous variable.

```{r}
#< task

#>
cor(dat$treat_im, dat$posterior_belief_part_hkust)
```

We find a negative correlation between the endogenous variable `posterior_belief_part_hkust` and the treatment indicator, which means the **relevancy condition** checks out.

Next up is the **exogeneity condition**. The **exogeneity condition** can not be checked statistically, as we do not observe the error term. This means that we can't just compute the correlation as we did for the **relevancy condition** above. However, this should also be satisfied if the treatment assignment in the study was indeed well randomized. Whether a student received truthful information in part two of the experiment was completely random and independent of individual characteristics in our experiment, which means that **exogeneity** of the instrument is given.

Last but not least we have got the **exclusion condition**. The question here is, whether information treatment only affected participation through the channel of posterior beliefs and not directly. In the last exercise we estimated a negative treatment effect of roughly 2.7 percentage points on participation.

#< quiz "Exclusion condition"
question: Does this violate the exclusion condition?
sc:
- yes
- no*
success: Great, your answer is correct!
failure: Try again.
#>

This is a bit tricky, because as mentioned, we did estimate an effect of truthful information on participation in the reduced form last exercise. That being said, the experimental treatment did not include incentives to join the protest or directly persuaded subjects to take part. It is therefore very likely that the treatment effect in the reduced form is attributed to the change of beliefs we observed in the first stage estimates, so treatment changed the subjects beliefs, which in turn moved participation. Thus, the **exclusion condition** is also satisfied and the direction adjusted treatment indicator is a valid instrument for prior beliefs.  

Since we found a valid instrument that allows us to estimate the causal effect of posterior beliefs on participation ,we can start with the first stage of two stage least squares.

**Task:** Fill in the blanks to regress the endogenous variable on the instrument.

```{r}
#< fill_in
first_stage<-lm(___ ~ ___ + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat)
#>
first_stage<-lm(posterior_belief_part_hkust ~ treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat)
```

Since we are dealing with heterogeneity in the treatment effect, we need to control for the interaction between prior beliefs regarding actual participation, and the dummy `above17`, indicating whether the prior guess on planned participation was higher than the true 17%. You might have noticed that this first stage estimate is exactly the same specification we used in the penultimate exercise. We combine first stage and reduced form estimates to identify the causal effect in this exercise. 
Next we estimate the constructed variable again by predicting beliefs with the first stage model.

**Task:** Fill in the blanks to predict the posterior beliefs using the first stage model.

```{r}
#< fill_in
dat<-dat%>%
  mutate(belief_instrument = ___)
#>
dat<-dat%>%
  mutate(belief_instrument = predict(first_stage))
```

Last but not least, we estimate the second stage, using the constructed variable.

**Task:** Fill in the blanks to estimate the second stage. 

```{r}
#< fill_in
second_stage <- lm(___ ~ ___ + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat)

modelsummary(list("Participation" = second_stage),
             output = "kableExtra",
             vcov = "hc1",
             coef_omit = c(1,3,4,5),
             coef_rename = c("belief_instrument" = "Posterior belief"),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "Table 6: Two stage estimates of Protest Participation")
#>
second_stage <- lm(part ~ belief_instrument + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat)

modelsummary(list("Participation" = second_stage),
             output = "kableExtra",
             vcov = "hc1",
             coef_omit = c(1,3,4,5),
             coef_rename = c("belief_instrument" = "Posterior belief"),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "Table 6: Two stage estimates of Protest Participation")
```

We estimate a significant causal effect of posterior beliefs amounting to approximately minus 0.553 percentage points on participation. A one percentage point rise in posterior beliefs causes participation to decrease by 0.553 percentage points.
 
#< quiz "Two-stage estimate"
question: This means we observe a game of strategic...
sc:
- complements
- substitutes*
success: Great, your answer is correct!
failure: Try again.
#>

Higher beliefs on turnout leading to lower individual participation strongly suggests **strategic substitutability**, as we already suspected in the last exercise when estimating the treatment effect on participation in the reduced form. This contradicts recent work claiming that **strategic complementarity** characterizes protests (Gehlbach et al. 2016). Before we jump to premature conclusions though, let us finish the two stage estimates. 

Performing the instrumental variable regression by hand does yield correct coefficient estimates, however the standard errors are not accurate. 
Using an existing function to estimate the instrumental variable regression is much easier anyway, so let us repeat the process using `Ã¬v_robust()` from the `estimatr` package. As shown in the example below,  the formula in the `iv_robust()` function consists of two parts separated by `|`. To the left we specify the original formula including the endogenous variable. On the right, we replace the endogenous variables with our instruments, keeping control variables. 

**Task:** Check the chunk below.

```{r}
#< task_notest
library(estimatr)

reg1 <- iv_robust(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = dat, se_type = "classical")
#>
```

In the last two exercises we have consistently seen offsetting treatment effects, differing in sign between higher and lower priors. Let us compare the effects among the two subgroups once again, estimating them separately.

**Task** Fill in the blanks to estimate the effect on protest participation for higher and lower priors respectively.

```{r}
#< fill_in
reg2 <- iv_robust(part ~ ___ + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | ___ + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 0), se_type = "classical")

reg3 <- iv_robust(part ~ ___ + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | ___ + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 1), se_type = "classical")

modelsummary(list("All Subjects" = reg1, "Prior below truth" = reg2, "Prior above truth" = reg3),
             output = "kableExtra",
             coef_omit = c(1,3,4,5),
             coef_rename = c("posterior_belief_part_hkust" = "Posterior belief"),
             gof_map = c("nobs"),
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "Table 6: Two stage estimates of Protest Participation")
#>
reg2 <- iv_robust(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 0), se_type = "classical")

reg3 <- iv_robust(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 1), se_type = "classical")

modelsummary(list("All Subjects" = reg1, "Prior below truth" = reg2, "Prior above truth" = reg3),
             output = "kableExtra",
             coef_omit = c(1,3,4,5),
             coef_rename = c("posterior_belief_part_hkust" = "Posterior belief"),
             gof_map = c("nobs"),
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "Table 6: Two stage estimates of Protest Participation")

```

As evidenced by the coefficients, we do not find a difference in sign in the causal effect of beliefs on participation. The effect is arguably a bit stronger (more negative) for subjects who initially overestimated the planned participation, however we find **strategic substitutability** in both subgroups.

As we already did in the last two exercises, we check randomization and for possible confounders by adding two panels, one including a multitude of control variables, and the other one removing extreme outliers. 

**Task** Check the chunk below.

```{r}
#< task_notest
#Panel B
control <- c("gender", "birth_year", "monthly_income_total_w3", "realest_totalowned_w3", "father_educ_hsabove_w123", "mother_educ_hsabove_w123", "hs_language_english", "hkgeneration_w123")

formula_im <- gf(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + {control} | treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + {control})
formula <- gf(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + {control} | treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 + {control})

reg4 <- iv_robust(formula_im, data = dat, se_type = "classical")
reg5 <- iv_robust(formula, data = filter(dat, above17 == 0), se_type = "classical")
reg6 <- iv_robust(formula, data = filter(dat, above17 == 1), se_type = "classical")

#Panel C
reg7 <- iv_robust(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | treat_im + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, !is.na(belief_planned_part_hkust_tr)), se_type = "classical")
reg8 <- iv_robust(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 0 & !is.na(belief_planned_part_hkust_tr)), se_type = "classical")
reg9 <- iv_robust(part ~ posterior_belief_part_hkust + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17 | treat + prior_belief_part_hkust + above17 + prior_belief_part_hkustXabove17, data = filter(dat, above17 == 1 & !is.na(belief_planned_part_hkust_tr)), se_type = "classical")

modelsummary(list("Panel A: Baseline" = list("All Subjects" = reg1, "Prior below truth" = reg2, "Prior above truth" = reg3), "Panel B: With controls" = list("All Subjects" = reg4,"Prior below truth" = reg5,"Prior above truth" = reg6), "Panel C: Trimmed" = list("All Subjects" = reg7, "Prior below truth" = reg8, "Prior above truth" = reg9)), 
         output = "kableExtra",
         coef_rename = c("guess_july1_2016_partust_w3pos" = "Posterior belief"),
         coef_omit = "^(?!.*posterior_belief_part_hkust)",
         gof_map = NA,
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
         shape = "rbind",
         title = " Table 6: Two stage estimates of Protest Participation")%>%
  add_header_above(c(" " = 1, "Participated in 2016 July 1 march" = 3))
#>
```

Adding control variables in panel B does not meaningfully alter the estimates compared to the baseline specification. This once again shows that the experiment is well randomized and we could not find confounders by controlling for individual characteristics. Removing extreme priors also had a marginal effect on coefficient for the causal effect of posterior beliefs on participation, though it has become more negative, reaffirming our claim of **strategic substitutability**. 

## Exercise 7. Conclusion

<br/>

In the experiment we estimated the causal effect of beliefs regarding other peoples participation on one's own, by exploiting exogenous variation in beliefs.

In the first stage we found offsetting treatment effects on beliefs: Treated subjects, who underestimated planned protest participation, corrected their beliefs positively, while subjects, who were more optimistic, corrected their beliefs negatively, with both groups moving towards the true 17%.
In the reduced form estimates, we found that truthful information affected participation negatively, though we did find offsetting effects in our two subsamples once more. For lower priors treatment affected turnout negatively, while for higher priors treatment increased participation. This points towards **strategic substitutability**, as students who received the message that more people were planning to go than they anticipated, tended not to show up themselves, while the opposite is true for students who were too optimistic in the first place. 
The two stage estimates confirm this, as we found a significant negative effect among both subgroups, indicating that higher beliefs cause a decrease in participation.

Both strategies for political parties in Hong Kong to motivate people to protest might still be viable options depending on individual characteristics. That being said, we find it to be more effective to signal to potential protesters that not enough people will attend. In a game of **strategic substitutes** a certain level of participation produces a public good, which in turn incentivizes **free-riding**, where an individual is profiting from others costly participation without facing the threat of sanctions. On average, advertising for the protest using an optimistic forecast regarding participation discourages individuals to participate, according to our findings.

The question is, whether we can generalize our findings. As described in the political context earlier, Hong Kong's democracy movement does share a lot of similarities with other antiauthoritarian movements across time and space. Namely, these characteristics are the struggle for democratic concessions and civil liberties, high turnout inspite possible crackdowns, with the protests remaining largely peaceful. This ensures that there is at least some degree of external validity.  
However, it must be acknowledged ,that even in the July 1st protest the causal effect on participation might vary for different levels of turnout, possibly even differing in sign, which would constitute a game of **strategic complements** instead (Cantoni et al., 2019). This would explain why we found indicators of both **strategic substitutability** and **strategic complementarity** in figure 1.

Our findings contradict recent theoretical work on this subject nonetheless, as the decision to protest is largely seen as a game of **strategic complements**, and does not allow for **strategic substitutability**. The results of this interactive analysis show, that **strategic substitutability** in the setting of antiauthoritarian protests is indeed possible and theoretical models on protest participation should allow for a setting of **strategic substitutes**. 


## Exercise 8. References


### Literature

Salciccioli, J.D., Crutain, Y., Komorowski, M., Marshall, D.C. (2016): "Sensitivity Analysis and Model Validation"

Bueno de Mesquita, Ethan, (2010): "Regime Change and Revolutionary Entrepreneurs", American Political Science Review, 104, 446 - 466

Bulow, J. I., Geanakoplos, J. D., & Klemperer, P. D. (1985): "Multimarket Oligopoly: Strategic Substitutes and Complements", Journal of Political Economy, 93(3), 488â€“511.

Christoph Hanck, Martin Arnold, Alexander Gerber, and Martin Schmelzer (2023): "Introduction to Econometrics with R", "Chapter 6.1 Omitted Variable Bias".

Coffman, Coffman, Ericson (2017): "The Size of the  LGBT Population and the Magnitude of Anti-Gay Sentiment are Substantially Underestimated".

D. Cantoni, D. Y. Yang, N. Yuchtman, Y. J. Zhang (2019): "Protests as Strategic Games: Experimental Evidence from Hong Kongs Antiauthoritarian Movement". 

Deaton, Angus and Cartwright, Nancy (2018): "Understanding and Misunderstanding Randomized Controlled Trials".  

DellaVigna and Gentzkow (2010): "Persuasion: Empirical Evidence".

Federal Foreign Office of Germany (2020): "One country, two systems â€“ historic compromise and a formula for Hong Kongâ€™s future", accessed on the 30th of July 2023, <https://www.auswaertiges-amt.de/en/aussenpolitik/laenderinformationen/hongkong-node/special-status-hongkong/2239304>

Francis L. F. Lee, Samson Yuen, Gary Tang and Edmund W. Cheng (2019): "Hong Kongâ€™s Summer of Uprising: From Anti-Extradition to Anti-Authoritarian Protest", The China Review Vol. 19, No. 4, 1-32. 

Gehlbach, Sonin, Svolik (2016): "Formal Models of Nondemocratic Politics", Annual Review of Political Science 19, 565-584.

Jessie Yeung, CNN (2021): "Prominent Hong Kong civil rights group disbands, citing government pressure", accessed on the 30th of July 2023, <https://edition.cnn.com/2021/08/15/asia/hong-kong-chrf-disbands-intl-hnk/index.html>

Kranz (2022): "Empirical Economics with R: Instrumental Variable Estimation, Potential Outcomes, and the Impact of Intensive Job Counseling"

Kranz (2022): "Empirical Economics with R: A deeper dive into linear regression and the estimation of causal effects"

Palfrey, Thomas, and Howard Rosenthal (1984): â€œParticipation and the Provision of Discrete Public Goods: A Strategic Analysisâ€, Journal of Public Economics, 24, 171â€“193.

Rieber (2021): "EinfÃ¼hrung in die KausalitÃ¤t"

Time, Amy Gunia (2019): "A Brief History of Protest in Post-Handover Hong Kong", accessed on the 20th of July 2023,
<https://time.com/5606212/hong-kong-history-mass-demonstrations-protest/>

Varadhan R, Seeger JD (2013): "Estimation and Reporting of Heterogeneity of Treatment Effects".


### Packages

Baptiste Auguie, Anton Antonov (2017). gridExtra: Miscellaneous Functions for "Grid" Graphics. R package version 2.3

Graeme Blair et al. (2022): Fast Estimators for Design-Based Inference. R package version 1.0.0

Hadley Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.

Hadley Wickham, Romain FranÃ§ois, Lionel Henry and Kirill MÃ¼ller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7. https://CRAN.R-project.org/package=dplyr

Hadley Wickham and Evan Miller (2021). haven: Import and Export 'SPSS', 'Stata' and 'SAS' Files. R package version 2.4.3. https://CRAN.R-project.org/package=haven
  
Hadley Wickham (2021). tidyr: Tidy Messy Data. R package version 1.1.4. https://CRAN.R-project.org/package=tidyr
  
Hao Zhu (2021). kableExtra: Construct Complex Table with 'kable' and Pipe Syntax. R package version 1.3.4. https://CRAN.R-project.org/package=kableExtra 

Sebastian Kranz. glueformula: string interpolation to build regression formulas. R package version 0.1.0

Sebastian Kranz (2020). RTutor: Interactive R problem sets with automatic testing of solutions and automatic hints. R package version 2020.11.25.

Vincent Arel-Bundock (2021). modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. R package version 0.9.4. https://CRAN.R-project.org/package=modelsummary



## Exercise 9. Appendix

<br/>

Load the data set again and prepare the experimental sample in the chunk below.

**Task:** Check the chunk.

```{r}
#< task_notest
data <- readRDS("data/data2.rds")
dat<-data%>%
  filter(followup_postjuly1st_w3 == 1 & !is.na(posterior_belief_part_hkust))
#>
```

### A1: Treatment effects on posterior beliefs among the HK population

In addition to the effects regarding other students participation, we also assess how beliefs on the participation among the entire Hong Kong population change after treatment.

**Task:** Check the chunk below. 

```{r}
#< task_notest
dat<-dat%>%
  mutate(above17 = ifelse(belief_planned_part_hkust >=17 & !is.na(belief_planned_part_hkust),1,0),
         prior_belief_part_totalXabove17 = prior_belief_part_total * above17)
# Panel A
reg1 <- lm(posterior_belief_part_total ~ treat_im + prior_belief_part_total + above17 + prior_belief_part_totalXabove17, data = dat)
reg2 <- lm(posterior_belief_part_total ~ treat + prior_belief_part_total + above17 + prior_belief_part_totalXabove17, data = filter(dat, above17 == 0))
reg3 <- lm(posterior_belief_part_total ~ treat + prior_belief_part_total + above17 + prior_belief_part_totalXabove17, data = filter(dat, above17 == 1))
#Panel B
control <- c("gender", "birth_year", "monthly_income_total_w3", "realest_totalowned_w3", "father_educ_hsabove_w123", "mother_educ_hsabove_w123", "hs_language_english", "hkgeneration_w123")
formula_im <- gf(posterior_belief_part_total ~ treat_im + prior_belief_part_total + above17 + prior_belief_part_totalXabove17 + {control})
reg4<-lm(formula_im, data = dat)
formula <- gf(posterior_belief_part_total ~ treat + prior_belief_part_total + above17 + prior_belief_part_totalXabove17 + {control})
reg5<-lm(formula, data = filter(dat, belief_planned_part_hkust < 17))
reg6<-lm(formula, data = filter(dat, belief_planned_part_hkust >= 17))
#Panel C
q05<-quantile(dat$belief_planned_part_hkust, 0.05)
q95<-quantile(dat$belief_planned_part_hkust, 0.95)
dat<-dat%>%
  mutate(belief_planned_part_hkust_tr = ifelse(belief_planned_part_hkust < q05 | belief_planned_part_hkust > q95, NA,
                                                belief_planned_part_hkust))
dat_trimmed<-dat%>%
  filter(!is.na(belief_planned_part_hkust_tr))
reg7<-lm(posterior_belief_part_total ~ treat_im + prior_belief_part_total + above17 + prior_belief_part_totalXabove17, data = dat_trimmed)
reg8<-lm(posterior_belief_part_total ~ treat + prior_belief_part_total + above17 + prior_belief_part_totalXabove17, data = filter(dat_trimmed, belief_planned_part_hkust < 17))
reg9<-lm(posterior_belief_part_total ~ treat + prior_belief_part_total + above17 + prior_belief_part_totalXabove17, data = filter(dat_trimmed, belief_planned_part_hkust >= 17))

modelsummary(list("Panel A: Baseline" = list("All Subjects" = reg1, "Prior below truth " = reg2, "Prior above truth " = reg3), "Panel B: With Controls" = list("All Subjects" = reg4, "Prior below truth " = reg5, "Prior above truth " = reg6), "Panel C: Trimmed" = list("All Subjects" = reg7, "Prior below truth " = reg8, "Prior above truth " = reg9)), 
         output = "kableExtra",
         vcov = "hc1",
         coef_omit = "^(?!.*treat)",
         coef_rename = c("treat_im" = "Treatment (direction adj.)", "treat" = "Treatment"),
         gof_map = NA,
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
         shape = "rbind",
         title = "A1: Treatment Effect on Posterior Beliefs")%>%
  add_header_above(c(" " = 1, "Posterior belief on participation among HK Population (#)" = 3))
#>
```

The estimated effects follow a similar trend in comparison to subjects change in beliefs regarding other students participation. We estimate a positive monotonic treatment effect and a offsetting effects for higher and lower priors. The effects are less significant, though, as the coefficients for higher priors are not significant in any specification at all. 


### A2: Heterogeneous treatment effects by prior beliefs (first stage)

As we did with the reduced form, we can also check whether the heterogeneity in the treatment effect on posterior beliefs is attributed to variation in beliefs or individual characterics.

```{r}
#< task_notest
fit <- lm(belief_planned_part_hkust ~ az_democratic_support + az_independence + az_ntlidentity + az_statusquo + az_polrecent + az_polapproach + participate_umbrella_w3 + vote_legco_2016_party_democ_w3 + planned_part + az_game_identity + demosis_donation_above0_w3 + az_preference_risk + az_preference_time + az_preference_altruism + az_preference_reciprocity + az_preference_redist + az_big5_openness + az_big5_agreebleness + az_big5_conscientious + az_big5_neuroticism + az_big5_extraversion + az_cognitive_iq + az_cognitive_gpa + az_demogra_hhecon + az_demogra_ownecon + az_demogra_childenv + az_belief_institution + az_belief_protesteff + az_srec_democratic_hkust + az_sres_independence_hkust + az_sres_ntlidentity_hkust + az_sres_statusquo_hkust + az_sres_approach_hkust + az_social_polnetwork + az_social_sociability + az_srec_democratic_fri + az_sres_independence_fri + az_sres_ntlidentity_fri + az_sres_statusquo_fri + az_sres_approach_fri + az_media_frequency + az_media_source + az_pol_interest + az_pol_knowledge + gender + birth_year + religion_none, data = dat, na.action = na.exclude)

dat <- dat%>%
  mutate(treatXbelief_planned = treat * belief_planned_part_hkust,
         predicted_belief_planned_part = predict(fit),
         residual_belief_planned_part = resid(fit),
         treatXpredicted = treat * predicted_belief_planned_part,
         treatXresidual = treat * residual_belief_planned_part)

reg1<-lm(posterior_belief_part_hkust ~ treat + belief_planned_part_hkust + treatXbelief_planned, data = dat)
reg2<-lm(posterior_belief_part_hkust ~ treat + predicted_belief_planned_part + treatXpredicted, data = dat)
reg3<-lm(posterior_belief_part_hkust ~ treat + predicted_belief_planned_part + treatXpredicted + residual_belief_planned_part + treatXresidual, data = dat)

modelsummary(list(reg1, reg2, reg3),
             output = "kableExtra",
             vcov = "hc1",
             coef_omit = c(1,3,5,7),
             coef_rename = c("treat" = "Treatment", "treatXbelief_planned" = "Treatment x Prior beliefs", "treatXpredicted" = "Treatment x predicted prior beliefs", "treatXresidual" = "Treatment x residual prior beliefs"),
             gof_map = c("nobs", "r.squared"),
             stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01),
             title = "A2: Heterogeneous Treatment Effects by Prior Beliefs")
#>
```

Consistent with our estimates from the main part, we find that individual characteristics do not solely account for heterogeneity in treatment effects on posterior beliefs. We observe a large and significant interaction between treatment and residuals, this unexplained variation can be attributed to a variation in beliefs.
